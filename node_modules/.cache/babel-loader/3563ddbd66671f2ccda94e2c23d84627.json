{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { pow } from '../ops/pow';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\nexport class AdamOptimizer extends Optimizer {\n  constructor(learningRate, beta1, beta2) {\n    let epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    super();\n    this.learningRate = learningRate;\n    this.beta1 = beta1;\n    this.beta2 = beta2;\n    this.epsilon = epsilon;\n    this.accumulatedFirstMoment = [];\n    this.accumulatedSecondMoment = [];\n    tidy(() => {\n      // accB* will be updated by batch.\n      this.accBeta1 = scalar(beta1).variable();\n      this.accBeta2 = scalar(beta2).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const varNames = Array.isArray(variableGradients) ? variableGradients.map(v => v.name) : Object.keys(variableGradients);\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const oneMinusAccBeta2 = sub(1, this.accBeta2);\n      varNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n\n        if (this.accumulatedSecondMoment[i] == null) {\n          this.accumulatedSecondMoment[i] = {\n            originalName: `${name}/v`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const secondMoment = this.accumulatedSecondMoment[i].variable;\n        const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n        const newSecondMoment = add(mul(secondMoment, this.beta2), mul(square(gradient), 1 - this.beta2));\n        const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n        const biasCorrectedSecondMoment = div(newSecondMoment, oneMinusAccBeta2);\n        firstMoment.assign(newFirstMoment);\n        secondMoment.assign(newSecondMoment);\n        const newValue = add(mul(div(biasCorrectedFirstMoment, add(sqrt(biasCorrectedSecondMoment), this.epsilon)), -this.learningRate), value);\n        value.assign(newValue);\n      });\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n      this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose();\n    this.accBeta2.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n\n    if (this.accumulatedSecondMoment != null) {\n      dispose(this.accumulatedSecondMoment.map(v => v.variable));\n    }\n  }\n\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    tidy(() => {\n      this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));\n      this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));\n    });\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedFirstMoment = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedSecondMoment = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon']);\n  }\n\n}\n/** @nocollapse */\n\nAdamOptimizer.className = 'Adam'; // Note: Name matters for Python compatibility.\n\nregisterClass(AdamOptimizer);","map":{"version":3,"sources":["../../../../../../tfjs-core/src/optimizers/adam_optimizer.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,OAAR,EAAiB,IAAjB,QAA4B,YAA5B;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,MAAR,QAAqB,eAArB;AACA,SAAQ,IAAR,QAAmB,aAAnB;AACA,SAAQ,MAAR,QAAqB,eAArB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,SAAR,QAAwB,mBAAxB;AACA,SAAoB,aAApB,QAA+E,kBAA/E;AAIA,SAAQ,SAAR,QAA2C,aAA3C;AAEA,OAAM,MAAO,aAAP,SAA6B,SAA7B,CAAsC;EAS1C,WAAA,CACc,YADd,EAC8C,KAD9C,EAEc,KAFd,EAE6D;IAAA,IAAtB,OAAsB,uEAAJ,IAAI;IAC3D;IAFY,KAAA,YAAA,GAAA,YAAA;IAAgC,KAAA,KAAA,GAAA,KAAA;IAChC,KAAA,KAAA,GAAA,KAAA;IAAyB,KAAA,OAAA,GAAA,OAAA;IAL/B,KAAA,sBAAA,GAA8C,EAA9C;IACA,KAAA,uBAAA,GAA+C,EAA/C;IAMN,IAAI,CAAC,MAAK;MACR;MACA,KAAK,QAAL,GAAgB,MAAM,CAAC,KAAD,CAAN,CAAc,QAAd,EAAhB;MACA,KAAK,QAAL,GAAgB,MAAM,CAAC,KAAD,CAAN,CAAc,QAAd,EAAhB;IACD,CAJG,CAAJ;;IAMA,IAAI,OAAO,IAAI,IAAf,EAAqB;MACnB,KAAK,OAAL,GAAe,MAAM,CAAC,OAAP,CAAe,OAAf,EAAf;IACD;EACF;;EAED,cAAc,CAAC,iBAAD,EAAkD;IAC9D,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,GAAlB,CAAsB,CAAC,IAAI,CAAC,CAAC,IAA7B,CADa,GAEb,MAAM,CAAC,IAAP,CAAY,iBAAZ,CAFJ;IAGA,IAAI,CAAC,MAAK;MACR,MAAM,gBAAgB,GAAG,GAAG,CAAC,CAAD,EAAI,KAAK,QAAT,CAA5B;MACA,MAAM,gBAAgB,GAAG,GAAG,CAAC,CAAD,EAAI,KAAK,QAAT,CAA5B;MAEA,QAAQ,CAAC,OAAT,CAAiB,CAAC,IAAD,EAAO,CAAP,KAAY;QAC3B,MAAM,KAAK,GAAG,MAAM,CAAC,mBAAP,CAA2B,IAA3B,CAAd;QACA,MAAM,SAAS,GAAG,KAAlB;;QACA,IAAI,KAAK,sBAAL,CAA4B,CAA5B,KAAkC,IAAtC,EAA4C;UAC1C,KAAK,sBAAL,CAA4B,CAA5B,IAAiC;YAC/B,YAAY,EAAE,GAAG,IAAI,IADU;YAE/B,QAAQ,EAAE,IAAI,CAAC,MAAM,SAAS,CAAC,KAAD,CAAT,CAAiB,QAAjB,CAA0B,SAA1B,CAAP;UAFiB,CAAjC;QAID;;QACD,IAAI,KAAK,uBAAL,CAA6B,CAA7B,KAAmC,IAAvC,EAA6C;UAC3C,KAAK,uBAAL,CAA6B,CAA7B,IAAkC;YAChC,YAAY,EAAE,GAAG,IAAI,IADW;YAEhC,QAAQ,EAAE,IAAI,CAAC,MAAM,SAAS,CAAC,KAAD,CAAT,CAAiB,QAAjB,CAA0B,SAA1B,CAAP;UAFkB,CAAlC;QAID;;QAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,CAAD,CAAjB,CAAqB,MADR,GAEb,iBAAiB,CAAC,IAAD,CAFrB;;QAGA,IAAI,QAAQ,IAAI,IAAhB,EAAsB;UACpB;QACD;;QAED,MAAM,WAAW,GAAG,KAAK,sBAAL,CAA4B,CAA5B,EAA+B,QAAnD;QACA,MAAM,YAAY,GAAG,KAAK,uBAAL,CAA6B,CAA7B,EAAgC,QAArD;QAEA,MAAM,cAAc,GAChB,GAAG,CAAC,GAAG,CAAC,WAAD,EAAc,KAAK,KAAnB,CAAJ,EAA+B,GAAG,CAAC,QAAD,EAAW,IAAI,KAAK,KAApB,CAAlC,CADP;QAEA,MAAM,eAAe,GACjB,GAAG,CAAC,GAAG,CAAC,YAAD,EAAe,KAAK,KAApB,CAAJ,EACC,GAAG,CAAC,MAAM,CAAC,QAAD,CAAP,EAAmB,IAAI,KAAK,KAA5B,CADJ,CADP;QAIA,MAAM,wBAAwB,GAAG,GAAG,CAAC,cAAD,EAAiB,gBAAjB,CAApC;QACA,MAAM,yBAAyB,GAC3B,GAAG,CAAC,eAAD,EAAkB,gBAAlB,CADP;QAGA,WAAW,CAAC,MAAZ,CAAmB,cAAnB;QACA,YAAY,CAAC,MAAb,CAAoB,eAApB;QAEA,MAAM,QAAQ,GACV,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,wBAAD,EACC,GAAG,CAAC,IAAI,CAAC,yBAAD,CAAL,EAAkC,KAAK,OAAvC,CADJ,CAAJ,EAEC,CAAC,KAAK,YAFP,CAAJ,EAGC,KAHD,CADP;QAKA,KAAK,CAAC,MAAN,CAAa,QAAb;MACD,CA7CD;MA+CA,KAAK,QAAL,CAAc,MAAd,CAAqB,GAAG,CAAC,KAAK,QAAN,EAAgB,KAAK,KAArB,CAAxB;MACA,KAAK,QAAL,CAAc,MAAd,CAAqB,GAAG,CAAC,KAAK,QAAN,EAAgB,KAAK,KAArB,CAAxB;IACD,CArDG,CAAJ;IAsDA,KAAK,mBAAL;EACD;;EAED,OAAO,GAAA;IACL,KAAK,QAAL,CAAc,OAAd;IACA,KAAK,QAAL,CAAc,OAAd;;IAEA,IAAI,KAAK,sBAAL,IAA+B,IAAnC,EAAyC;MACvC,OAAO,CAAC,KAAK,sBAAL,CAA4B,GAA5B,CAAgC,CAAC,IAAI,CAAC,CAAC,QAAvC,CAAD,CAAP;IACD;;IACD,IAAI,KAAK,uBAAL,IAAgC,IAApC,EAA0C;MACxC,OAAO,CAAC,KAAK,uBAAL,CAA6B,GAA7B,CAAiC,CAAC,IAAI,CAAC,CAAC,QAAxC,CAAD,CAAP;IACD;EACF;;EAEe,MAAV,UAAU,GAAA;IACd;IACA,MAAM,SAAS,GACX,CAAC,GAAG,KAAK,sBAAT,EAAiC,GAAG,KAAK,uBAAzC,CADJ;IAEA,OAAO,CAAC,MAAM,KAAK,cAAL,EAAP,EAA8B,MAA9B,CACH,SAAS,CAAC,GAAV,CAAc,CAAC,KAAK;MAAC,IAAI,EAAE,CAAC,CAAC,YAAT;MAAuB,MAAM,EAAE,CAAC,CAAC;IAAjC,CAAL,CAAf,CADG,CAAP;EAED;;EAEe,MAAV,UAAU,CAAC,YAAD,EAA4B;IAC1C,YAAY,GAAG,MAAM,KAAK,iBAAL,CAAuB,YAAvB,CAArB;IACA,IAAI,CAAC,MAAK;MACR,KAAK,QAAL,CAAc,MAAd,CAAqB,GAAG,CAAC,KAAK,KAAN,EAAa,KAAK,WAAL,GAAmB,CAAhC,CAAxB;MACA,KAAK,QAAL,CAAc,MAAd,CAAqB,GAAG,CAAC,KAAK,KAAN,EAAa,KAAK,WAAL,GAAmB,CAAhC,CAAxB;IACD,CAHG,CAAJ;IAKA,MAAM,aAAa,GAAG,YAAY,CAAC,MAAb,GAAsB,CAA5C;IACA,MAAM,SAAS,GAAG,KAAlB;IACA,KAAK,sBAAL,GACI,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,aAAtB,EAAqC,GAArC,CAAyC,CAAC,KAAK;MACJ,YAAY,EAAE,CAAC,CAAC,IADZ;MAEJ,QAAQ,EAAE,CAAC,CAAC,MAAF,CAAS,QAAT,CACN,SADM;IAFN,CAAL,CAA1C,CADJ;IAMA,KAAK,uBAAL,GACI,YAAY,CAAC,KAAb,CAAmB,aAAnB,EAAkC,aAAa,GAAG,CAAlD,EACK,GADL,CACS,CAAC,KAAK;MACJ,YAAY,EAAE,CAAC,CAAC,IADZ;MAEJ,QAAQ,EAAE,CAAC,CAAC,MAAF,CAAS,QAAT,CAAkB,SAAlB;IAFN,CAAL,CADV,CADJ;EAMD;;EAED,SAAS,GAAA;IACP,OAAO;MACL,gBAAgB,KAAK,YADhB;MAEL,SAAS,KAAK,KAFT;MAGL,SAAS,KAAK,KAHT;MAIL,WAAW,KAAK;IAJX,CAAP;EAMD;EAED;;;EACiB,OAAV,UAAU,CACb,GADa,EACoB,MADpB,EACsC;IACrD,OAAO,IAAI,GAAJ,CACH,MAAM,CAAC,cAAD,CADH,EACqB,MAAM,CAAC,OAAD,CAD3B,EACsC,MAAM,CAAC,OAAD,CAD5C,EAEH,MAAM,CAAC,SAAD,CAFH,CAAP;EAGD;;AA/IyC;AAC1C;;AACO,aAAA,CAAA,SAAA,GAAY,MAAZ,C,CAAqB;;AA+I9B,aAAa,CAAC,aAAD,CAAb","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {pow} from '../ops/pow';\nimport {scalar} from '../ops/scalar';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {Variable} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\nexport class AdamOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'Adam';  // Note: Name matters for Python compatibility.\n  private accBeta1: Variable;\n  private accBeta2: Variable;\n\n  private accumulatedFirstMoment: OptimizerVariable[] = [];\n  private accumulatedSecondMoment: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon: number = null) {\n    super();\n    tidy(() => {\n      // accB* will be updated by batch.\n      this.accBeta1 = scalar(beta1).variable();\n      this.accBeta2 = scalar(beta2).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const oneMinusAccBeta2 = sub(1, this.accBeta2);\n\n      varNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n        if (this.accumulatedSecondMoment[i] == null) {\n          this.accumulatedSecondMoment[i] = {\n            originalName: `${name}/v`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ?\n            variableGradients[i].tensor :\n            variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const secondMoment = this.accumulatedSecondMoment[i].variable;\n\n        const newFirstMoment =\n            add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n        const newSecondMoment =\n            add(mul(secondMoment, this.beta2),\n                mul(square(gradient), 1 - this.beta2));\n\n        const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n        const biasCorrectedSecondMoment =\n            div(newSecondMoment, oneMinusAccBeta2);\n\n        firstMoment.assign(newFirstMoment);\n        secondMoment.assign(newSecondMoment);\n\n        const newValue =\n            add(mul(div(biasCorrectedFirstMoment,\n                        add(sqrt(biasCorrectedSecondMoment), this.epsilon)),\n                    -this.learningRate),\n                value);\n        value.assign(newValue);\n      });\n\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n      this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    this.accBeta1.dispose();\n    this.accBeta2.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n    if (this.accumulatedSecondMoment != null) {\n      dispose(this.accumulatedSecondMoment.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    tidy(() => {\n      this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));\n      this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));\n    });\n\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedFirstMoment =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedSecondMoment =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['beta1'], config['beta2'],\n        config['epsilon']);\n  }\n}\nregisterClass(AdamOptimizer);\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}