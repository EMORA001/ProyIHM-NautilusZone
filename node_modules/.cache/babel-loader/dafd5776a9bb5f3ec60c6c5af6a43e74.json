{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape) {\n    this.built = true;\n  } // TODO(cais): Implement activityRegularizer getter.\n\n\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  } // TODO(cais): Implement setter for trainableWeights.\n\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  } // TODO(cais): Implement setter for nonTrainableWeights.\n\n\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  } // TODO(cais): Implement getUpdatesFor().\n\n\n  get losses() {\n    return this.layer.losses;\n  } // TODO(cais): Implement getLossesFor().\n\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n\n    if (inputShape.length < 3) {\n      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` + `input shape ${JSON.stringify(inputShape)}`);\n    }\n\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n\n      const rnnOutputs = rnn(step, inputs, [], false\n      /* goBackwards */\n      , null\n      /* mask */\n      , null\n      /* constants */\n      , false\n      /* unroll */\n      , true\n      /* needPerStepOutputs */\n      );\n      const y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n\n      return y;\n    });\n  }\n\n}\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n\n    outputShape = outputShape;\n\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n\n    const additionalInputs = [];\n    const additionalSpecs = [];\n\n    if (initialState != null) {\n      const numStates = initialState.length;\n\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n\n      let states;\n\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n\n        y = y[0];\n        yRev = yRev[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n\n      let output;\n\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      } // TODO(cais): Properly set learning phase.\n\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n\n        return [output].concat(states);\n      }\n\n      return output;\n    });\n  }\n\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n\n    let outputMask;\n\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  } // TODO(cais): Implement constraints().\n\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` + `present is not supported yet.`);\n    } // tslint:disable-next-line:no-any\n\n\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n\n}\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"sources":["../../../../../../tfjs-layers/src/layers/wrappers.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ,MAAmB,yBAAnB;AACA,SAAQ,SAAR,QAAwB,WAAxB;AACA,SAAQ,SAAR,EAAmB,KAAnB,EAAqC,cAArC,QAA0D,oBAA1D;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAuC,+BAAvC,QAA6E,wBAA7E;AAGA,OAAO,KAAK,aAAZ,MAA+B,wBAA/B;AACA,SAAQ,kBAAR,EAA4B,mBAA5B,QAAsD,sBAAtD;AAGA,SAAQ,GAAR,EAAkB,eAAlB,QAAwC,aAAxC;AACA,SAAQ,WAAR,QAA0B,iBAA1B;AASA;;;;;;AAMG;;AACH,OAAM,MAAgB,OAAhB,SAAgC,KAAhC,CAAqC;EAGzC,WAAA,CAAY,IAAZ,EAAkC;IAChC;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAM,IAAN;IACA,KAAK,KAAL,GAAa,IAAI,CAAC,KAAlB;EACD;;EAED,KAAK,CAAC,UAAD,EAA0B;IAC7B,KAAK,KAAL,GAAa,IAAb;EACD,CAjBwC,CAmBzC;;;EAEa,IAAT,SAAS,GAAA;IACX;IACA;IACA;IACA,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;MACtB,OAAO,KAAK,KAAL,CAAW,SAAlB;IACD,CAFD,MAEO;MACL,OAAO,KAAP;IACD;EACF;;EAEY,IAAT,SAAS,CAAC,KAAD,EAAe;IAC1B;IACA;IACA;IACA,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;MACtB,KAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;IACD;EACF;;EAEmB,IAAhB,gBAAgB,GAAA;IAClB,OAAO,KAAK,KAAL,CAAW,gBAAlB;EACD,CA3CwC,CA4CzC;;;EAEuB,IAAnB,mBAAmB,GAAA;IACrB,OAAO,KAAK,KAAL,CAAW,mBAAlB;EACD,CAhDwC,CAiDzC;;;EAEW,IAAP,OAAO,GAAA;IACT;IACA,OAAQ,KAAK,KAAL,CAAmB,QAA3B;EACD,CAtDwC,CAwDzC;;;EAEU,IAAN,MAAM,GAAA;IACR,OAAO,KAAK,KAAL,CAAW,MAAlB;EACD,CA5DwC,CA8DzC;;;EAEA,UAAU,GAAA;IACR,OAAO,KAAK,KAAL,CAAW,UAAX,EAAP;EACD;;EAED,UAAU,CAAC,OAAD,EAAkB;IAC1B,KAAK,KAAL,CAAW,UAAX,CAAsB,OAAtB;EACD;;EAED,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,SAAS;QACP,aAAa,KAAK,KAAL,CAAW,YAAX,EADN;QAEP,UAAU,KAAK,KAAL,CAAW,SAAX;MAFH;IAD8B,CAAzC;IAMA,MAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;IACA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;IACA,OAAO,MAAP;EACD;;EAED,4BAA4B,CAAC,KAAD,EAAe;IACzC,MAAM,4BAAN,CAAmC,KAAnC;;IACA,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;MACtB,KAAK,KAAL,CAAW,4BAAX,CAAwC,KAAxC;IACD;EACF;EAED;;;EACiB,OAAV,UAAU,CACb,GADa,EAEb,MAFa,EAGiC;IAAA,IAA9C,aAA8C,uEAA9B,EAA8B;IAChD,MAAM,WAAW,GAAG,MAAM,CAAC,OAAD,CAA1B;IACA,MAAM,KAAK,GAAG,WAAW,CAAC,WAAD,EAAc,aAAd,CAAzB;IACA,OAAO,MAAM,CAAC,OAAD,CAAb;IACA,MAAM,SAAS,GAAG;MAAC;IAAD,CAAlB;IACA,MAAM,CAAC,MAAP,CAAc,SAAd,EAAyB,MAAzB;IACA,OAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;EACD;;AAtGwC;AAyG3C,OAAM,MAAO,eAAP,SAA+B,OAA/B,CAAsC;EAG1C,WAAA,CAAY,IAAZ,EAAkC;IAChC,MAAM,IAAN;IACA,KAAK,eAAL,GAAuB,IAAvB;EACD;;EAED,KAAK,CAAC,UAAD,EAA0B;IAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;;IACA,IAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;MACzB,MAAM,IAAI,UAAJ,CACF,mEAAA,GACA,eAAe,IAAI,CAAC,SAAL,CAAe,UAAf,CAA0B,EAFvC,CAAN;IAGD;;IACD,KAAK,SAAL,GAAiB,CAAC;MAAC,KAAK,EAAE;IAAR,CAAD,CAAjB;IACA,MAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;IACA,IAAI,CAAC,KAAK,KAAL,CAAW,KAAhB,EAAuB;MACrB,KAAK,KAAL,CAAW,KAAX,CAAiB,eAAjB;MACA,KAAK,KAAL,CAAW,KAAX,GAAmB,IAAnB;IACD;;IACD,MAAM,KAAN,CAAY,UAAZ;EACD;;EAED,kBAAkB,CAAC,UAAD,EAA0B;IAC1C,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;IACA,MAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;IACA,MAAM,gBAAgB,GAClB,KAAK,KAAL,CAAW,kBAAX,CAA8B,eAA9B,CADJ;IAEA,MAAM,SAAS,GAAG,UAAU,CAAC,CAAD,CAA5B;IACA,OAAO,CAAC,gBAAgB,CAAC,CAAD,CAAjB,EAAsB,SAAtB,EAAiC,MAAjC,CAAwC,gBAAgB,CAAC,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;EACD;;EAED,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf;MACA,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B,CAFe,CAGf;MACA;MACA;;MACA,MAAM,IAAI,GAAoB,CAAC,MAAD,EAAiB,MAAjB,KAAqC;QACjE;QACA;QACA;QACA;QACA,MAAM,MAAM,GAAG,mBAAmB,CAAC,KAAK,KAAL,CAAW,IAAX,CAAgB,MAAhB,EAAwB,MAAxB,CAAD,CAAlC;QACA,OAAO,CAAC,MAAD,EAAS,EAAT,CAAP;MACD,CAPD;;MAQA,MAAM,UAAU,GACZ,GAAG,CAAC,IAAD,EAAO,MAAP,EAAe,EAAf,EAAmB;MAAM;MAAzB,EAA4C;MAAK;MAAjD,EACC;MAAK;MADN,EACuB;MAAM;MAD7B,EAEC;MAAK;MAFN,CADP;MAIA,MAAM,CAAC,GAAG,UAAU,CAAC,CAAD,CAApB,CAlBe,CAmBf;MACA;;MACA,OAAO,CAAP;IACD,CAtBU,CAAX;EAuBD;;AAzDyC;AAC1C;;AACO,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA2DT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAEA,OAAM,SAAU,2BAAV,CAAsC,KAAtC,EAAoD;EACxD,aAAa,CAAC,yBAAd,CACI,+BADJ,EACqC,wBADrC,EAC+D,KAD/D;AAED;AAkBD,MAAM,gCAAgC,GAA2B,QAAjE;AAEA,OAAM,MAAO,aAAP,SAA6B,OAA7B,CAAoC;EAWxC,WAAA,CAAY,IAAZ,EAAwC;IACtC,MAAM,IAAN,EADsC,CAGtC;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IACA,MAAM,WAAW,GAAG,IAAI,CAAC,KAAL,CAAW,SAAX,EAApB;IACA,MAAM,QAAQ,GAA6B,EAA3C;IACA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;IACA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;IACA,KAAK,YAAL,GAAoB,WAAW,CAAC,QAAD,CAA/B;IACA,WAAW,CAAC,aAAD,CAAX,GACI,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;IAEA,MAAM,QAAQ,GAA6B,EAA3C;IACA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;IACA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;IACA,KAAK,aAAL,GAAqB,WAAW,CAAC,QAAD,CAAhC;IACA,KAAK,YAAL,CAAkB,IAAlB,GAAyB,aAAa,KAAK,YAAL,CAAkB,IAAxD;IACA,KAAK,aAAL,CAAmB,IAAnB,GAA0B,cAAc,KAAK,aAAL,CAAmB,IAA3D;IAEA,KAAK,SAAL,GAAiB,IAAI,CAAC,SAAL,KAAmB,SAAnB,GACb,gCADa,GAEb,IAAI,CAAC,SAFT;IAGA,2BAA2B,CAAC,KAAK,SAAN,CAA3B;;IACA,IAAI,IAAI,CAAC,OAAT,EAAkB;MAChB,MAAM,IAAI,mBAAJ,CACF,iEADE,CAAN;IAED;;IACD,KAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,QAA5B;IACA,KAAK,eAAL,GAAuB,IAAI,CAAC,KAAL,CAAW,eAAlC;IACA,KAAK,WAAL,GAAmB,IAAI,CAAC,KAAL,CAAW,WAA9B;IACA,KAAK,eAAL,GAAuB,IAAvB;IACA,KAAK,UAAL,GAAkB,IAAlB;IACA,KAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,SAA5B;IACA,KAAK,YAAL,GAAoB,IAApB;EACD;;EAEY,IAAT,SAAS,GAAA;IACX,OAAO,KAAK,UAAZ;EACD;;EAEY,IAAT,SAAS,CAAC,KAAD,EAAe;IAC1B;IACA;IACA;IACA,KAAK,UAAL,GAAkB,KAAlB;;IACA,IAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;MAC7B,KAAK,YAAL,CAAkB,SAAlB,GAA8B,KAA9B;IACD;;IACD,IAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;MAC9B,KAAK,aAAL,CAAmB,SAAnB,GAA+B,KAA/B;IACD;EACF;;EAED,UAAU,GAAA;IACR,OAAO,KAAK,YAAL,CAAkB,UAAlB,GAA+B,MAA/B,CACH,KAAK,aAAL,CAAmB,UAAnB,EADG,CAAP;EAED;;EAED,UAAU,CAAC,OAAD,EAAkB;IAC1B,MAAM,UAAU,GAAG,OAAO,CAAC,MAA3B;IACA,MAAM,cAAc,GAAG,IAAI,CAAC,KAAL,CAAW,UAAU,GAAG,CAAxB,CAAvB;IACA,KAAK,YAAL,CAAkB,UAAlB,CAA6B,OAAO,CAAC,KAAR,CAAc,CAAd,EAAiB,cAAjB,CAA7B;IACA,KAAK,aAAL,CAAmB,UAAnB,CAA8B,OAAO,CAAC,KAAR,CAAc,cAAd,CAA9B;EACD;;EAED,kBAAkB,CAAC,UAAD,EAA0B;IAC1C,IAAI,WAAW,GACX,KAAK,YAAL,CAAkB,kBAAlB,CAAqC,UAArC,CADJ;;IAEA,IAAI,EAAE,KAAK,CAAC,OAAN,CAAc,WAAd,KAA8B,KAAK,CAAC,OAAN,CAAc,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;MAClE,WAAW,GAAG,CAAC,WAAD,CAAd;IACD;;IACD,WAAW,GAAG,WAAd;IAEA,IAAI,WAAJ;IACA,IAAI,YAAJ;IACA,IAAI,UAAJ;;IACA,IAAI,KAAK,WAAT,EAAsB;MACpB,UAAU,GAAG,WAAW,CAAC,KAAZ,CAAkB,CAAlB,CAAb;MACA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;IACD,CAHD,MAGO;MACL,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;IACD;;IACD,WAAW,GAAG,WAAd;;IACA,IAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;MAC/B,WAAW,CAAC,WAAW,CAAC,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;MACA,YAAY,GAAG,CAAC,WAAD,CAAf;IACD,CAHD,MAGO,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;MACjC,YAAY,GAAG,CAAC,WAAD,EAAc,WAAW,CAAC,KAAZ,EAAd,CAAf;IACD,CAFM,MAEA;MACL,YAAY,GAAG,CAAC,WAAD,CAAf;IACD;;IAED,IAAI,KAAK,WAAT,EAAsB;MACpB,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;QAC1B,OAAO,YAAY,CAAC,MAAb,CAAoB,UAApB,EAAgC,MAAhC,CAAuC,UAAU,CAAC,KAAX,EAAvC,CAAP;MACD;;MACD,OAAO,CAAC,WAAD,EAAc,MAAd,CAAqB,UAArB,EAAiC,MAAjC,CAAwC,UAAU,CAAC,KAAX,EAAxC,CAAP;IACD;;IACD,OAAO,aAAa,CAAC,gBAAd,CAA+B,YAA/B,CAAP;EACD;;EAED,KAAK,CACD,MADC,EAED,MAFC,EAEc;IACjB,IAAI,YAAY,GACZ,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,cAAD,CADlC;IAEA,IAAI,SAAS,GACT,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,WAAD,CADlC;;IAEA,IAAI,MAAM,IAAI,IAAd,EAAoB;MAClB,MAAM,GAAG,EAAT;IACD;;IACD,MAAM,YAAY,GACd,eAAe,CAAC,MAAD,EAAS,YAAT,EAAuB,SAAvB,EAAkC,KAAK,YAAvC,CADnB;IAEA,MAAM,GAAG,YAAY,CAAC,MAAtB;IACA,YAAY,GAAG,YAAY,CAAC,YAA5B;IACA,SAAS,GAAG,YAAY,CAAC,SAAzB;;IAEA,IAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;MACzB,YAAY,GAAI,MAAsC,CAAC,KAAvC,CAA6C,CAA7C,CAAhB;MACA,MAAM,GAAI,MAAsC,CAAC,CAAD,CAAhD;IACD;;IAED,IAAI,CAAC,YAAY,IAAI,IAAhB,IAAwB,YAAY,CAAC,MAAb,KAAwB,CAAjD,KACA,SAAS,IAAI,IADjB,EACuB;MACrB,OAAO,MAAM,KAAN,CAAY,MAAZ,EAAoB,MAApB,CAAP;IACD;;IACD,MAAM,gBAAgB,GAAiC,EAAvD;IACA,MAAM,eAAe,GAAgB,EAArC;;IACA,IAAI,YAAY,IAAI,IAApB,EAA0B;MACxB,MAAM,SAAS,GAAG,YAAY,CAAC,MAA/B;;MACA,IAAI,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;QACrB,MAAM,IAAI,UAAJ,CACF,wDACA,wDADA,GAEA,sBAHE,CAAN;MAID;;MACD,MAAM,CAAC,cAAD,CAAN,GAAyB,YAAzB;MACA,gBAAgB,CAAC,IAAjB,CAAsB,GAAG,YAAzB;MACA,MAAM,UAAU,GAAI,YAA6C,CACzC,GADJ,CACQ,KAAK,IAAI,IAAI,SAAJ,CAAc;QAAC,KAAK,EAAE,KAAK,CAAC;MAAd,CAAd,CADjB,CAApB;MAEA,KAAK,YAAL,CAAkB,SAAlB,GAA8B,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,SAAS,GAAG,CAAhC,CAA9B;MACA,KAAK,aAAL,CAAmB,SAAnB,GAA+B,UAAU,CAAC,KAAX,CAAiB,SAAS,GAAG,CAA7B,CAA/B;MACA,eAAe,CAAC,IAAhB,CAAqB,GAAG,UAAxB;IACD;;IACD,IAAI,SAAS,IAAI,IAAjB,EAAuB;MACrB,MAAM,IAAI,mBAAJ,CACF,0DACA,kBAFE,CAAN;IAGD;;IAED,MAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAD,CAAhB,YAA+B,cAAxD;;IACA,KAAK,MAAM,MAAX,IAAqB,gBAArB,EAAuC;MACrC,IAAI,MAAM,YAAY,cAAlB,KAAqC,gBAAzC,EAA2D;QACzD,MAAM,IAAI,UAAJ,CACF,0DACA,yDAFE,CAAN;MAGD;IACF;;IAED,IAAI,gBAAJ,EAAsB;MACpB;MACA,MAAM,SAAS,GAAG,CAAC,MAAD,EAAS,MAAT,CAAgB,gBAAhB,CAAlB;MACA,MAAM,aAAa,GAAG,KAAK,SAAL,CAAe,MAAf,CAAsB,eAAtB,CAAtB,CAHoB,CAIpB;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;MACA,MAAM,iBAAiB,GAAG,KAAK,SAA/B;MACA,KAAK,SAAL,GAAiB,aAAjB;MACA,MAAM,MAAM,GACR,MAAM,KAAN,CAAY,SAAZ,EAAsD,MAAtD,CADJ;MAEA,KAAK,SAAL,GAAiB,iBAAjB;MACA,OAAO,MAAP;IACD,CAnBD,MAmBO;MACL,OAAO,MAAM,KAAN,CAAY,MAAZ,EAAoB,MAApB,CAAP;IACD;EACF;;EAED,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;IAC1C,OAAO,IAAI,CAAC,MAAK;MACf,MAAM,YAAY,GAAG,MAAM,CAAC,cAAD,CAA3B;MAEA,IAAI,CAAJ;MACA,IAAI,IAAJ;;MACA,IAAI,YAAY,IAAI,IAApB,EAA0B;QACxB,CAAC,GAAG,KAAK,YAAL,CAAkB,IAAlB,CAAuB,MAAvB,EAA+B,MAA/B,CAAJ;QACA,IAAI,GAAG,KAAK,aAAL,CAAmB,IAAnB,CAAwB,MAAxB,EAAgC,MAAhC,CAAP;MACD,CAHD,MAGO;QACL,MAAM,YAAY,GAAG,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,YAAY,CAAC,MAAb,GAAsB,CAA5C,CAArB;QACA,MAAM,aAAa,GAAG,YAAY,CAAC,KAAb,CAAmB,YAAY,CAAC,MAAb,GAAsB,CAAzC,CAAtB;QACA,CAAC,GAAG,KAAK,YAAL,CAAkB,IAAlB,CACA,MADA,EACQ,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;UAAC,YAAY,EAAE;QAAf,CAAtB,CADR,CAAJ;QAEA,IAAI,GAAG,KAAK,aAAL,CAAmB,IAAnB,CACH,MADG,EACK,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;UAAC,YAAY,EAAE;QAAf,CAAtB,CADL,CAAP;MAED;;MAED,IAAI,MAAJ;;MACA,IAAI,KAAK,WAAT,EAAsB;QACpB,IAAI,KAAK,CAAC,OAAN,CAAc,CAAd,CAAJ,EAAsB;UACpB,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,EAAW,MAAX,CAAmB,IAAiB,CAAC,KAAlB,CAAwB,CAAxB,CAAnB,CAAT;QACD,CAFD,MAEO,CACN;;QACD,CAAC,GAAI,CAAc,CAAC,CAAD,CAAnB;QACA,IAAI,GAAI,IAAiB,CAAC,CAAD,CAAzB;MACD;;MAED,IAAI,KAAK,eAAT,EAA0B;QACxB,IAAI,GAAG,GAAG,CAAC,OAAJ,CAAY,IAAZ,EAA4B,CAA5B,CAAP;MACD;;MAED,IAAI,MAAJ;;MACA,IAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;QAC/B,MAAM,GAAG,CAAC,CAAC,WAAF,CAAc,CAAC,CAAD,EAAc,IAAd,CAAd,CAAT;MACD,CAFD,MAEO,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;QACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;MACD,CAFM,MAEA,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;QACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAZ,CAAT;MACD,CAFM,MAEA,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;QACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;MACD,CAFM,MAEA,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;QACjC,MAAM,GAAG,CAAC,CAAD,EAAc,IAAd,CAAT;MACD,CA1Cc,CA4Cf;;;MACA,IAAI,KAAK,WAAT,EAAsB;QACpB,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;UAC1B,OAAQ,MAAmB,CAAC,MAApB,CAA2B,MAA3B,CAAR;QACD;;QACD,OAAO,CAAC,MAAD,EAAmB,MAAnB,CAA0B,MAA1B,CAAP;MACD;;MACD,OAAO,MAAP;IACD,CApDU,CAAX;EAqDD;;EAED,WAAW,CAAC,MAAD,EAAyB;IAClC,KAAK,YAAL,CAAkB,WAAlB;IACA,KAAK,aAAL,CAAmB,WAAnB;EACD;;EAED,KAAK,CAAC,UAAD,EAA0B;IAC7B,SAAS,CAAC,KAAK,YAAL,CAAkB,IAAnB,EAAyB,MAAK;MACrC,KAAK,YAAL,CAAkB,KAAlB,CAAwB,UAAxB;IACD,CAFQ,CAAT;IAGA,SAAS,CAAC,KAAK,aAAL,CAAmB,IAApB,EAA0B,MAAK;MACtC,KAAK,aAAL,CAAmB,KAAnB,CAAyB,UAAzB;IACD,CAFQ,CAAT;IAGA,KAAK,KAAL,GAAa,IAAb;EACD;;EAED,WAAW,CAAC,MAAD,EAA0B,IAA1B,EAAgD;IAEzD,IAAI,KAAK,CAAC,OAAN,CAAc,IAAd,CAAJ,EAAyB;MACvB,IAAI,GAAG,IAAI,CAAC,CAAD,CAAX;IACD;;IACD,IAAI,UAAJ;;IACA,IAAI,KAAK,eAAT,EAA0B;MACxB,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;QAC1B,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;MACD,CAFD,MAEO;QACL,UAAU,GAAG,IAAb;MACD;IACF,CAND,MAMO;MACL,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;QAC1B,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;MACD,CAFD,MAEO;QACL,UAAU,GAAG,IAAb;MACD;IACF;;IACD,IAAI,KAAK,WAAT,EAAsB;MACpB,MAAM,MAAM,GAAG,KAAK,YAAL,CAAkB,MAAjC;MACA,MAAM,SAAS,GAAa,MAAM,CAAC,GAAP,CAAW,KAAK,IAAI,IAApB,CAA5B;;MACA,IAAI,KAAK,CAAC,OAAN,CAAc,UAAd,CAAJ,EAA+B;QAC7B,OAAO,UAAU,CAAC,MAAX,CAAkB,SAAlB,EAA6B,MAA7B,CAAoC,SAApC,CAAP;MACD,CAFD,MAEO;QACL,OAAO,CAAC,UAAD,EAAa,MAAb,CAAoB,SAApB,EAA+B,MAA/B,CAAsC,SAAtC,CAAP;MACD;IACF,CARD,MAQO;MACL,OAAO,UAAP;IACD;EACF;;EAEmB,IAAhB,gBAAgB,GAAA;IAClB,OAAO,KAAK,YAAL,CAAkB,gBAAlB,CAAmC,MAAnC,CACH,KAAK,aAAL,CAAmB,gBADhB,CAAP;EAED;;EAEsB,IAAnB,mBAAmB,GAAA;IACrB,OAAO,KAAK,YAAL,CAAkB,mBAAlB,CAAsC,MAAtC,CACH,KAAK,aAAL,CAAmB,mBADhB,CAAP;EAED,CAvTuC,CAyTxC;;;EAEA,4BAA4B,CAAC,KAAD,EAAe;IACzC,MAAM,4BAAN,CAAmC,KAAnC;;IACA,IAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;MAC7B,KAAK,YAAL,CAAkB,4BAAlB,CAA+C,KAA/C;IACD;;IACD,IAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;MAC9B,KAAK,aAAL,CAAmB,4BAAnB,CAAgD,KAAhD;IACD;EACF;;EAED,SAAS,GAAA;IACP,MAAM,MAAM,GAA6B;MACvC,aAAa,KAAK;IADqB,CAAzC,CADO,CAIP;;IACA,MAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;IACA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;IACA,OAAO,MAAP;EACD;EAED;;;EACiB,OAAV,UAAU,CACb,GADa,EAEb,MAFa,EAEmB;IAClC,MAAM,QAAQ,GACV,WAAW,CAAC,MAAM,CAAC,OAAD,CAAP,CADf;IAEA,OAAO,MAAM,CAAC,OAAD,CAAb,CAHkC,CAIlC;;IACA,IAAI,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;MAClC,MAAM,IAAI,mBAAJ,CACF,6DAAA,GACA,+BAFE,CAAN;IAGD,CATiC,CAUlC;;;IACA,MAAM,SAAS,GAAyB,MAAxC;IACA,SAAS,CAAC,OAAD,CAAT,GAAqB,QAArB;IACA,OAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;EACD;;AAhWuC;AACxC;;AACO,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgWT,aAAa,CAAC,aAAd,CAA4B,aAA5B","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {InputSpec, Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {BidirectionalMergeMode, Shape, VALID_BIDIRECTIONAL_MERGE_MODES} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {RegularizerFn, RnnStepFunction} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nimport {rnn, RNN, standardizeArgs} from './recurrent';\nimport {deserialize} from './serialization';\n\nexport declare interface WrapperLayerArgs extends LayerArgs {\n  /**\n   * The layer to be wrapped.\n   */\n  layer: Layer;\n}\n\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport abstract class Wrapper extends Layer {\n  readonly layer: Layer;\n\n  constructor(args: WrapperLayerArgs) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    this.built = true;\n  }\n\n  // TODO(cais): Implement activityRegularizer getter.\n\n  get trainable(): boolean {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n\n  get updates(): Tensor[] {\n    // tslint:disable-next-line:no-any\n    return (this.layer as any)._updates;\n  }\n\n  // TODO(cais): Implement getUpdatesFor().\n\n  get losses(): RegularizerFn[] {\n    return this.layer.losses;\n  }\n\n  // TODO(cais): Implement getLossesFor().\n\n  getWeights(): Tensor[] {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights: Tensor[]): void {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig(),\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const layerConfig = config['layer'] as serialization.ConfigDict;\n    const layer = deserialize(layerConfig, customObjects) as Layer;\n    delete config['layer'];\n    const newConfig = {layer};\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\n\nexport class TimeDistributed extends Wrapper {\n  /** @nocollapse */\n  static className = 'TimeDistributed';\n  constructor(args: WrapperLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(\n          `TimeDistributed layer expects an input shape >= 3D, but received ` +\n          `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{shape: inputShape}];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape =\n        this.layer.computeOutputShape(childInputShape) as Shape;\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step: RnnStepFunction = (inputs: Tensor, states: Tensor[]) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs =\n          rnn(step, inputs, [], false /* goBackwards */, null /* mask */,\n              null /* constants */, false /* unroll */,\n              true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n\n  // TODO(cais): Implement detailed computeMask() logic.\n}\nserialization.registerClass(TimeDistributed);\n\nexport function checkBidirectionalMergeMode(value?: string): void {\n  generic_utils.checkStringTypeUnionValue(\n      VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\n\nexport declare interface BidirectionalLayerArgs extends WrapperLayerArgs {\n  /**\n   * The instance of an `RNN` layer to be wrapped.\n   */\n  layer: RNN;\n\n  /**\n   * Mode by which outputs of the forward and backward RNNs are\n   * combined. If `null` or `undefined`, the output will not be\n   * combined, they will be returned as an `Array`.\n   *\n   * If `undefined` (i.e., not provided), defaults to `'concat'`.\n   */\n  mergeMode?: BidirectionalMergeMode;\n}\n\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE: BidirectionalMergeMode = 'concat';\n\nexport class Bidirectional extends Wrapper {\n  /** @nocollapse */\n  static className = 'Bidirectional';\n  mergeMode: BidirectionalMergeMode;\n  private forwardLayer: RNN;\n  private backwardLayer: RNN;\n  private returnSequences: boolean;\n  private returnState: boolean;\n  private numConstants?: number;\n  private _trainable: boolean;\n\n  constructor(args: BidirectionalLayerArgs) {\n    super(args);\n\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict: serialization.ConfigDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict) as RNN;\n    layerConfig['goBackwards'] =\n        layerConfig['goBackwards'] === true ? false : true;\n    const backDict: serialization.ConfigDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict) as RNN;\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n\n    this.mergeMode = args.mergeMode === undefined ?\n        DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n        args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError(\n          'weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable(): boolean {\n    return this._trainable;\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights(): Tensor[] {\n    return this.forwardLayer.getWeights().concat(\n        this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights: Tensor[]): void {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    let layerShapes: Shape|Shape[] =\n        this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes as Shape];\n    }\n    layerShapes = layerShapes as Shape[];\n\n    let outputShape: Shape;\n    let outputShapes: Shape[];\n    let stateShape: Shape[];\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = (inputs as Tensor[] | SymbolicTensor[]).slice(1);\n      inputs = (inputs as Tensor[] | SymbolicTensor[])[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) &&\n        constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    const additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError(\n            'When passing `initialState` to a Bidrectional RNN, ' +\n            'the state should be an Array containing the states of ' +\n            'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = (initialState as Array<Tensor|SymbolicTensor>)\n                             .map(state => new InputSpec({shape: state.shape}));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError(\n          'Support for constants in Bidirectional layers is not ' +\n          'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError(\n            'The initial state of a Bidirectional layer cannot be ' +\n            'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output =\n          super.apply(fullInput as Tensor[] | SymbolicTensor[], kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n\n      let y: Tensor|Tensor[];\n      let yRev: Tensor|Tensor[];\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: forwardState}));\n        yRev = this.backwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: backwardState}));\n      }\n\n      let states: Tensor[];\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat((yRev as Tensor[]).slice(1));\n        } else {\n        }\n        y = (y as Tensor[])[0];\n        yRev = (yRev as Tensor[])[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev as Tensor, 1);\n      }\n\n      let output: Tensor|Tensor[];\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y as Tensor, yRev as Tensor]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y as Tensor, yRev as Tensor));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode == null) {\n        output = [y as Tensor, yRev as Tensor];\n      }\n\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return (output as Tensor[]).concat(states);\n        }\n        return [output as Tensor].concat(states);\n      }\n      return output;\n    });\n  }\n\n  resetStates(states?: Tensor|Tensor[]): void {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask: Tensor|Tensor[];\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask: Tensor[] = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.forwardLayer.trainableWeights.concat(\n        this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.forwardLayer.nonTrainableWeights.concat(\n        this.backwardLayer.nonTrainableWeights);\n  }\n\n  // TODO(cais): Implement constraints().\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'mergeMode': this.mergeMode,\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    const rnnLayer =\n        deserialize(config['layer'] as serialization.ConfigDict) as RNN;\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\n          `Deserialization of a Bidirectional layer with numConstants ` +\n          `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig: {[key: string]: any} = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\nserialization.registerClass(Bidirectional);\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}