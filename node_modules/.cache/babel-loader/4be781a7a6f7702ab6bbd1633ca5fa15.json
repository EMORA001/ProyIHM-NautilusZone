{"ast":null,"code":"/**\n * @license\n * Copyright 2022 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util } from '../base';\nimport { Prod } from '../kernel_names';\nimport { cumprod } from '../ops/cumprod';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { transpose } from '../ops/transpose'; // Gradient for product operation on a single axis.\n\nfunction prodGradFn_(x, dy, axis) {\n  // The gradient tensor (dy) has a set of axes removed, so we create re-shaped\n  // versions (of size 1) for the removed axis; this supports broadcasting over\n  // those dimensions.\n  const expandedYShape = x.shape.slice();\n  expandedYShape[axis] = 1; // The actual gradient computation.\n\n  const expandedDy = reshape(dy, expandedYShape);\n  const xCumProd = cumprod(x, axis, true, false);\n  const xCumRevProd = cumprod(x, axis, true, true);\n  const dx = mul(xCumProd, xCumRevProd);\n  return mul(expandedDy, dx);\n} // Support gradients when the product is done on many axes at once.\n// This done py pushing all the axes on which the product is applied into a\n// single axis.\n\n\nfunction prodsGradFn_(x, dy, axis) {\n  // Move all axes for doing prod over to the end of the tensor.\n  const xRank = x.shape.length;\n  const finalProdAxis = xRank - axis.length;\n  const xPermutation = backend_util.getAxesPermutation(axis, xRank);\n  let permutedX = x;\n\n  if (xPermutation != null) {\n    permutedX = transpose(x, xPermutation);\n  } // Reshape all the prod dimensions into a single one, and do compute prod\n  // gradients on that.\n\n\n  const newShape = permutedX.shape.slice();\n  const removedShape = newShape.splice(xRank - axis.length, axis.length);\n  const endPartShape = removedShape.reduce((p, c) => p * c, 1);\n  newShape.push(endPartShape);\n  const reshapedPermutedX = permutedX.reshape(newShape);\n  let prodGrad = prodGradFn_(reshapedPermutedX, dy, finalProdAxis); // Undo the re-shaping now we have the dx vector, and permute back to\n  // original axes order.\n\n  prodGrad = prodGrad.reshape(permutedX.shape);\n\n  if (xPermutation != null) {\n    const undoPermutation = backend_util.getUndoAxesPermutation(xPermutation);\n    prodGrad = transpose(prodGrad, undoPermutation);\n  }\n\n  return prodGrad;\n} // Running example:\n// [\n//   [\n//     [3.0, 4.0],\n//     [5.0, 6.0],\n//     [7.0, 8.0]\n//   ],\n//   [\n//     [3.0, 5.0],\n//     [0.0, 6.0],\n//     [5.0, 6.0]\n//   ]\n// ]\n//\n\n\nexport const prodGradConfig = {\n  kernelName: Prod,\n  inputsToSave: ['x'],\n  gradFunc: (dy, saved, attrs) => {\n    const [x] = saved;\n    const {\n      axis\n    } = attrs;\n    let axisArr = [];\n\n    if (axis === undefined || axis === null) {\n      axisArr = x.shape.map((_, i) => i);\n    } else if (typeof axis === 'number') {\n      axisArr = [axis];\n    } else {\n      axisArr = axis;\n    }\n\n    return {\n      x: () => prodsGradFn_(x, dy, axisArr)\n    };\n  }\n};","map":{"version":3,"sources":["../../../../../../tfjs-core/src/gradients/Prod_grad.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,YAAR,QAA2B,SAA3B;AACA,SAAQ,IAAR,QAA8B,iBAA9B;AAEA,SAAQ,OAAR,QAAsB,gBAAtB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,OAAR,QAAsB,gBAAtB;AACA,SAAQ,SAAR,QAAwB,kBAAxB,C,CAGA;;AACA,SAAS,WAAT,CAAqB,CAArB,EAAgC,EAAhC,EAA4C,IAA5C,EAAwD;EACtD;EACA;EACA;EACA,MAAM,cAAc,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAvB;EACA,cAAc,CAAC,IAAD,CAAd,GAAuB,CAAvB,CALsD,CAOtD;;EACA,MAAM,UAAU,GAAG,OAAO,CAAC,EAAD,EAAK,cAAL,CAA1B;EACA,MAAM,QAAQ,GAAG,OAAO,CAAC,CAAD,EAAI,IAAJ,EAAU,IAAV,EAAgB,KAAhB,CAAxB;EACA,MAAM,WAAW,GAAG,OAAO,CAAC,CAAD,EAAI,IAAJ,EAAU,IAAV,EAAgB,IAAhB,CAA3B;EACA,MAAM,EAAE,GAAG,GAAG,CAAC,QAAD,EAAW,WAAX,CAAd;EACA,OAAO,GAAG,CAAC,UAAD,EAAa,EAAb,CAAV;AACD,C,CAED;AACA;AACA;;;AACA,SAAS,YAAT,CAAsB,CAAtB,EAAiC,EAAjC,EAA6C,IAA7C,EAA2D;EACzD;EACA,MAAM,KAAK,GAAG,CAAC,CAAC,KAAF,CAAQ,MAAtB;EACA,MAAM,aAAa,GAAG,KAAK,GAAG,IAAI,CAAC,MAAnC;EACA,MAAM,YAAY,GAAG,YAAY,CAAC,kBAAb,CAAgC,IAAhC,EAAsC,KAAtC,CAArB;EACA,IAAI,SAAS,GAAG,CAAhB;;EACA,IAAI,YAAY,IAAI,IAApB,EAA0B;IACxB,SAAS,GAAG,SAAS,CAAC,CAAD,EAAI,YAAJ,CAArB;EACD,CARwD,CAUzD;EACA;;;EACA,MAAM,QAAQ,GAAG,SAAS,CAAC,KAAV,CAAgB,KAAhB,EAAjB;EACA,MAAM,YAAY,GAAG,QAAQ,CAAC,MAAT,CAAgB,KAAK,GAAG,IAAI,CAAC,MAA7B,EAAqC,IAAI,CAAC,MAA1C,CAArB;EACA,MAAM,YAAY,GAAG,YAAY,CAAC,MAAb,CAAoB,CAAC,CAAD,EAAI,CAAJ,KAAU,CAAC,GAAG,CAAlC,EAAqC,CAArC,CAArB;EACA,QAAQ,CAAC,IAAT,CAAc,YAAd;EACA,MAAM,iBAAiB,GAAG,SAAS,CAAC,OAAV,CAAkB,QAAlB,CAA1B;EACA,IAAI,QAAQ,GAAG,WAAW,CAAC,iBAAD,EAAoB,EAApB,EAAwB,aAAxB,CAA1B,CAjByD,CAmBzD;EACA;;EACA,QAAQ,GAAG,QAAQ,CAAC,OAAT,CAAiB,SAAS,CAAC,KAA3B,CAAX;;EACA,IAAI,YAAY,IAAI,IAApB,EAA0B;IACxB,MAAM,eAAe,GAAG,YAAY,CAAC,sBAAb,CAAoC,YAApC,CAAxB;IACA,QAAQ,GAAG,SAAS,CAAC,QAAD,EAAW,eAAX,CAApB;EACD;;EACD,OAAO,QAAP;AACD,C,CAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,OAAO,MAAM,cAAc,GAAe;EACxC,UAAU,EAAE,IAD4B;EAExC,YAAY,EAAE,CAAC,GAAD,CAF0B;EAGxC,QAAQ,EAAE,CAAC,EAAD,EAAsB,KAAtB,EAAuC,KAAvC,KAA8D;IACtE,MAAM,CAAC,CAAD,IAAM,KAAZ;IACA,MAAM;MAAC;IAAD,IAAU,KAAhB;IACA,IAAI,OAAO,GAAG,EAAd;;IACA,IAAI,IAAI,KAAK,SAAT,IAAsB,IAAI,KAAK,IAAnC,EAAyC;MACvC,OAAO,GAAG,CAAC,CAAC,KAAF,CAAQ,GAAR,CAAY,CAAC,CAAD,EAAI,CAAJ,KAAU,CAAtB,CAAV;IACD,CAFD,MAEO,IAAI,OAAO,IAAP,KAAgB,QAApB,EAA8B;MACnC,OAAO,GAAG,CAAC,IAAD,CAAV;IACD,CAFM,MAEA;MACL,OAAO,GAAG,IAAV;IACD;;IACD,OAAO;MAAC,CAAC,EAAE,MAAM,YAAY,CAAC,CAAD,EAAI,EAAJ,EAAkB,OAAlB;IAAtB,CAAP;EACD;AAfuC,CAAnC","sourcesContent":["/**\n * @license\n * Copyright 2022 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '../base';\nimport {Prod, ProdAttrs} from '../kernel_names';\nimport {GradConfig, NamedAttrMap} from '../kernel_registry';\nimport {cumprod} from '../ops/cumprod';\nimport {mul} from '../ops/mul';\nimport {reshape} from '../ops/reshape';\nimport {transpose} from '../ops/transpose';\nimport {Tensor} from '../tensor';\n\n// Gradient for product operation on a single axis.\nfunction prodGradFn_(x: Tensor, dy: Tensor, axis: number): Tensor {\n  // The gradient tensor (dy) has a set of axes removed, so we create re-shaped\n  // versions (of size 1) for the removed axis; this supports broadcasting over\n  // those dimensions.\n  const expandedYShape = x.shape.slice();\n  expandedYShape[axis] = 1;\n\n  // The actual gradient computation.\n  const expandedDy = reshape(dy, expandedYShape);\n  const xCumProd = cumprod(x, axis, true, false);\n  const xCumRevProd = cumprod(x, axis, true, true);\n  const dx = mul(xCumProd, xCumRevProd);\n  return mul(expandedDy, dx);\n}\n\n// Support gradients when the product is done on many axes at once.\n// This done py pushing all the axes on which the product is applied into a\n// single axis.\nfunction prodsGradFn_(x: Tensor, dy: Tensor, axis: number[]): Tensor {\n  // Move all axes for doing prod over to the end of the tensor.\n  const xRank = x.shape.length;\n  const finalProdAxis = xRank - axis.length;\n  const xPermutation = backend_util.getAxesPermutation(axis, xRank);\n  let permutedX = x;\n  if (xPermutation != null) {\n    permutedX = transpose(x, xPermutation);\n  }\n\n  // Reshape all the prod dimensions into a single one, and do compute prod\n  // gradients on that.\n  const newShape = permutedX.shape.slice();\n  const removedShape = newShape.splice(xRank - axis.length, axis.length);\n  const endPartShape = removedShape.reduce((p, c) => p * c, 1);\n  newShape.push(endPartShape);\n  const reshapedPermutedX = permutedX.reshape(newShape);\n  let prodGrad = prodGradFn_(reshapedPermutedX, dy, finalProdAxis);\n\n  // Undo the re-shaping now we have the dx vector, and permute back to\n  // original axes order.\n  prodGrad = prodGrad.reshape(permutedX.shape);\n  if (xPermutation != null) {\n    const undoPermutation = backend_util.getUndoAxesPermutation(xPermutation);\n    prodGrad = transpose(prodGrad, undoPermutation);\n  }\n  return prodGrad;\n}\n\n// Running example:\n// [\n//   [\n//     [3.0, 4.0],\n//     [5.0, 6.0],\n//     [7.0, 8.0]\n//   ],\n//   [\n//     [3.0, 5.0],\n//     [0.0, 6.0],\n//     [5.0, 6.0]\n//   ]\n// ]\n//\nexport const prodGradConfig: GradConfig = {\n  kernelName: Prod,\n  inputsToSave: ['x'],\n  gradFunc: (dy: Tensor|Tensor[], saved: Tensor[], attrs: NamedAttrMap) => {\n    const [x] = saved;\n    const {axis} = (attrs as {}) as ProdAttrs;\n    let axisArr = [] as number[];\n    if (axis === undefined || axis === null) {\n      axisArr = x.shape.map((_, i) => i);\n    } else if (typeof axis === 'number') {\n      axisArr = [axis];\n    } else {\n      axisArr = axis;\n    }\n    return {x: () => prodsGradFn_(x, dy as Tensor, axisArr)};\n  }\n};\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}