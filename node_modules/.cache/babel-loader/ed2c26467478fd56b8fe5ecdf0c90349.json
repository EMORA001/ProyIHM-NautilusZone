{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\n\nfunction fusedDepthwiseConv2d_(_ref) {\n  let {\n    x,\n    filter,\n    strides,\n    pad,\n    dataFormat = 'NHWC',\n    dilations = [1, 1],\n    dimRoundingMode,\n    bias,\n    activation = 'linear',\n    preluActivationWeights,\n    leakyreluAlpha\n  } = _ref;\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` + `rank ${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` + `but got rank ${$filter.rank}.`);\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` + `(${x4D.shape[3]}) must match the inChannels dimension in ` + `filter ${$filter.shape[2]}.`);\n\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' + `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  conv_util.checkPadOnDimRoundingMode('fused depthwiseConv2d', pad, dimRoundingMode);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true\n  /* depthwise */\n  );\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy, saved) => {\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' + `greater than 1 are not yet supported. Got dilations ` + `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n\n    return [xDer, filterDer];\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport const depthwiseConv2d = op({\n  fusedDepthwiseConv2d_\n});","map":{"version":3,"sources":["../../../../../../../tfjs-core/src/ops/fused/depthwise_conv2d.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAqB,cAArB;AACA,SAAQ,UAAR,QAAyB,iBAAzB;AACA,SAAQ,oBAAR,QAA0F,oBAA1F;AAIA,SAAQ,cAAR,QAA6B,mBAA7B;AACA,SAAQ,eAAR,QAA8B,uBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,YAAtB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,OAAO,KAAK,cAAZ,MAAgC,mBAAhC;AACA,OAAO,KAAK,SAAZ,MAA2B,cAA3B;AACA,SAAQ,eAAe,IAAI,sBAA3B,QAAwD,qBAAxD;AACA,SAAQ,mCAAR,QAAkD,4CAAlD;AACA,SAAQ,kCAAR,QAAiD,2CAAjD;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,EAA+C,oBAA/C,EAAqE,UAArE,QAAsF,eAAtF;AACA,SAAQ,EAAR,QAAiB,cAAjB;AACA,SAAQ,OAAR,QAAsB,YAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAkDG;;AACH,SAAS,qBAAT,OAwBC;EAAA,IAxB2D;IAC1D,CAD0D;IAE1D,MAF0D;IAG1D,OAH0D;IAI1D,GAJ0D;IAK1D,UAAU,GAAG,MAL6C;IAM1D,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAN8C;IAO1D,eAP0D;IAQ1D,IAR0D;IAS1D,UAAU,GAAG,QAT6C;IAU1D,sBAV0D;IAW1D;EAX0D,CAwB3D;;EACC,IAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAChE,IAAI,MAAM,GAAG,sBAAsB,CAC/B,CAD+B,EAC5B,MAD4B,EACpB,OADoB,EACX,GADW,EACN,UADM,EACM,SADN,EACiB,eADjB,CAAnC;;IAEA,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;IACD;;IAED,OAAO,eAAe,CACX,MADW,EACH,UADG,EACS,sBADT,EACiC,cADjC,CAAtB;EAED;;EAED,MAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,iBAAT,EAA4B,SAA5B,CAA1B;EACA,MAAM,OAAO,GACT,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,iBAAnB,EAAsC,SAAtC,CADnB;EAGA,IAAI,GAAG,GAAG,EAAV;EACA,IAAI,YAAY,GAAG,KAAnB;;EACA,IAAI,EAAE,CAAC,IAAH,KAAY,CAAhB,EAAmB;IACjB,YAAY,GAAG,IAAf;IACA,GAAG,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,EAAE,CAAC,KAAH,CAAS,CAAT,CAAJ,EAAiB,EAAE,CAAC,KAAH,CAAS,CAAT,CAAjB,EAA8B,EAAE,CAAC,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;EACD;;EACD,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,IAAJ,KAAa,CADjB,EAEI,MAAM,gEAAA,GACF,QAAQ,GAAG,CAAC,IAAI,GAHxB;EAIA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,IAAR,KAAiB,CADrB,EAEI,MAAM,yDAAA,GACF,gBAAgB,OAAO,CAAC,IAAI,GAHpC;EAIA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,CAAU,CAAV,MAAiB,OAAO,CAAC,KAAR,CAAc,CAAd,CADrB,EAEI,MAAM,2DAAA,GACF,IAAI,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAY,2CADd,GAEF,UAAU,OAAO,CAAC,KAAR,CAAc,CAAd,CAAgB,GAJlC;;EAKA,IAAI,SAAS,IAAI,IAAjB,EAAuB;IACrB,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAZ;EACD;;EACD,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,8BAAV,CAAyC,OAAzC,EAAkD,SAAlD,CADJ,EAEI,MACI,sEACA,qBAAqB,OAAO,mBAAmB,SAAS,GAJhE;EAKA,SAAS,CAAC,yBAAV,CACI,uBADJ,EAC6B,GAD7B,EACkC,eADlC;EAEA,MAAM,QAAQ,GAAG,SAAS,CAAC,iBAAV,CACb,GAAG,CAAC,KADS,EACF,OAAO,CAAC,KADN,EACa,OADb,EACsB,SADtB,EACiC,GADjC,EACsC,eADtC,EAEb;EAAK;EAFQ,CAAjB;EAIA,IAAI,KAAJ;;EACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;IACA,CAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;IAEA,cAAc,CAAC,0BAAf,CAA0C,QAAQ,CAAC,QAAnD,EAA6D,KAAK,CAAC,KAAnE;EACD;;EAED,IAAI,uBAAJ;;EACA,IAAI,sBAAsB,IAAI,IAA9B,EAAoC;IAClC,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,uBADJ,CAAzC;EAED;;EAED,MAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;IAC7C,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,iBAAV,CAA4B,SAA5B,CADJ,EAEI,MAAM,gEACF,sDADE,GAEF,IAAI,SAAS,GAJrB;IAKA,MAAM,CAAC,OAAD,EAAU,GAAV,EAAe,CAAf,EAAkB,IAAlB,IAA0B,KAAhC;IAEA,MAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;IAEA,MAAM,IAAI,GAAG,kCAAkC,CAC1C,GAAgB,CAAC,KADyB,EAClB,YADkB,EACJ,OADI,EACiB,OADjB,EAE3C,GAF2C,EAEtC,SAFsC,EAE3B,eAF2B,CAA/C;IAGA,MAAM,SAAS,GAAG,mCAAmC,CACjD,GADiD,EAChC,YADgC,EACjB,OAAoB,CAAC,KADJ,EACW,OADX,EAEjD,GAFiD,EAE5C,SAF4C,EAEjC,eAFiC,CAArD;;IAIA,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,MAAM,OAAO,GAAG,oBAAoB,CAAC,KAAD,EAAQ,YAAR,CAApC;MACA,OAAO,CAAC,IAAD,EAAO,SAAP,EAAkB,OAAlB,CAAP;IACD;;IACD,OAAO,CAAC,IAAD,EAAO,SAAP,CAAP;EACD,CAtBD;;EAwBA,MAAM,MAAM,GAA+B;IACzC,CAAC,EAAE,GADsC;IAEzC,MAAM,EAAE,OAFiC;IAGzC,IAAI,EAAE,KAHmC;IAIzC,sBAAsB,EAAE;EAJiB,CAA3C;EAMA,MAAM,KAAK,GAA8B;IACvC,OADuC;IAEvC,GAFuC;IAGvC,UAHuC;IAIvC,SAJuC;IAKvC,eALuC;IAMvC,UANuC;IAOvC;EAPuC,CAAzC,CA7FD,CAuGC;EACA;;EACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,MAAM,QAAQ,GACV,UAAU,CAAC,CAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,KAAwD;MACjE;MACA,IAAI,GAAG,GAAsB,MAAM,CAAC,SAAP,CACzB,oBADyB,EACH,MADG,EAEzB,KAFyB,CAA7B;MAIA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,CAAD,CAAJ;;MAEA,IAAI,YAAJ,EAAkB;QAChB;QACA,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MAED;;MAED,OAAO;QAAC,KAAK,EAAE,GAAR;QAAa,QAAQ,EAAE;MAAvB,CAAP;IACD,CAfS,CADd;IAiBA,OAAO,QAAQ,CAAC,GAAD,EAAM,OAAN,CAAf;EACD,CAnBD,MAmBO;IACL,MAAM,gBAAgB,GAAG,UAAU,CAC/B,CAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,EAAgD,IAAhD,KAAsE;MACpE;MACA,IAAI,GAAG,GAAsB,MAAM,CAAC,SAAP,CACzB,oBADyB,EACH,MADG,EAEzB,KAFyB,CAA7B;MAIA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,EAAmB,IAAnB,CAAD,CAAJ;;MAEA,IAAI,YAAJ,EAAkB;QAChB;QACA,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MAED;;MAED,OAAO;QAAC,KAAK,EAAE,GAAR;QAAa,QAAQ,EAAE;MAAvB,CAAP;IACD,CAhB8B,CAAnC;IAkBA,OAAO,gBAAgB,CAAC,GAAD,EAAM,OAAN,EAAe,KAAf,CAAvB;EACD;AACF;;AACD,OAAO,MAAM,eAAe,GAAG,EAAE,CAAC;EAAC;AAAD,CAAD,CAA1B","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedDepthwiseConv2D, FusedDepthwiseConv2DAttrs, FusedDepthwiseConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport {depthwiseConv2d as unfusedDepthwiseConv2d} from '../depthwise_conv2d';\nimport {depthwiseConv2dNativeBackpropFilter} from '../depthwise_conv2d_native_backprop_filter';\nimport {depthwiseConv2dNativeBackpropInput} from '../depthwise_conv2d_native_backprop_input';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  const $filter =\n      convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n          `rank ${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n          `but got rank ${$filter.rank}.`);\n  util.assert(\n      x4D.shape[3] === $filter.shape[2],\n      () => `Error in fused depthwiseConv2d: number of input channels ` +\n          `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n          `filter ${$filter.shape[2]}.`);\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () =>\n          'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n          `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  conv_util.checkPadOnDimRoundingMode(\n      'fused depthwiseConv2d', pad, dimRoundingMode);\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode,\n      true /* depthwise */);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    const xDer = depthwiseConv2dNativeBackpropInput(\n        (x4D as Tensor4D).shape, dyActivation, $filter as Tensor4D, strides,\n        pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(\n        x4D as Tensor4D, dyActivation, ($filter as Tensor4D).shape, strides,\n        pad, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n    return [xDer, filterDer];\n  };\n\n  const inputs: FusedDepthwiseConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs: FusedDepthwiseConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as {} as NamedTensorMap,\n              attrs as {} as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as {} as NamedTensorMap,\n              attrs as {} as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const depthwiseConv2d = op({fusedDepthwiseConv2d_});\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}