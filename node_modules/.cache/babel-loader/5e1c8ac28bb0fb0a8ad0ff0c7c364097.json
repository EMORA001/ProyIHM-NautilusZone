{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Dilation2DBackpropInput, util } from '@tensorflow/tfjs-core';\nexport const dilation2DBackpropInputConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: _ref => {\n    let {\n      inputs,\n      backend,\n      attrs\n    } = _ref;\n    const {\n      x,\n      filter,\n      dy\n    } = inputs;\n    const {\n      strides,\n      pad,\n      dilations\n    } = attrs;\n    const cpuBackend = backend;\n    const $x = util.toNestedArray(x.shape, cpuBackend.data.get(x.dataId).values);\n    const $filter = util.toNestedArray(filter.shape, cpuBackend.data.get(filter.dataId).values);\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } = backend_util.computeDilation2DInfo(x.shape, filter.shape, strides, pad, 'NHWC'\n    /* dataFormat */\n    , dilations);\n    util.assert(dy.rank === outShape.length, () => `Error in ${Dilation2DBackpropInput}, dy ` + `must have the same rank as output ${outShape.length}, but got ` + `${dy.rank}`);\n    const $dy = util.toNestedArray(outShape, cpuBackend.data.get(dy.dataId).values); // The computed gradients has the same dimensions as the input:\n    // [batch, inputHeight, inputCols, inChannel]\n\n    const gradients = util.makeZerosNestedTypedArray(x.shape, x.dtype); // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hInMax = hBeg < 0 ? 0 : hBeg;\n            let wInMax = wBeg < 0 ? 0 : wBeg;\n\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n\n                    if (val > curVal) {\n                      curVal = val;\n                      hInMax = hIn;\n                      wInMax = wIn;\n                    }\n                  }\n                }\n              }\n            }\n\n            gradients[b][hInMax][wInMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(util.toTypedArray(gradients, x.dtype), x.shape, x.dtype);\n    return {\n      dataId,\n      shape: x.shape,\n      dtype: x.dtype\n    };\n  }\n};","map":{"version":3,"sources":["../../../../../../tfjs-backend-cpu/src/kernels/Dilation2DBackpropInput.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,YAAR,EAAuC,uBAAvC,EAAgG,IAAhG,QAA2G,uBAA3G;AAKA,OAAO,MAAM,6BAA6B,GAAiB;EACzD,UAAU,EAAE,uBAD6C;EAEzD,WAAW,EAAE,KAF4C;EAGzD,UAAU,EAAE,QAA6B;IAAA,IAA5B;MAAC,MAAD;MAAS,OAAT;MAAkB;IAAlB,CAA4B;IACvC,MAAM;MAAC,CAAD;MAAI,MAAJ;MAAY;IAAZ,IACF,MADJ;IAEA,MAAM;MAAC,OAAD;MAAU,GAAV;MAAe;IAAf,IAA4B,KAAlC;IACA,MAAM,UAAU,GAAG,OAAnB;IAEA,MAAM,EAAE,GACJ,IAAI,CAAC,aAAL,CACI,CAAC,CAAC,KADN,EACa,UAAU,CAAC,IAAX,CAAgB,GAAhB,CAAoB,CAAC,CAAC,MAAtB,EAA8B,MAD3C,CADJ;IAKA,MAAM,OAAO,GAAG,IAAI,CAAC,aAAL,CACI,MAAM,CAAC,KADX,EAEI,UAAU,CAAC,IAAX,CAAgB,GAAhB,CAAoB,MAAM,CAAC,MAA3B,EAAmC,MAFvC,CAAhB;IAKA,MAAM;MACJ,SADI;MAEJ,QAFI;MAGJ,OAHI;MAIJ,UAJI;MAKJ,SALI;MAMJ,QANI;MAOJ,OAPI;MAQJ,YARI;MASJ,WATI;MAUJ,YAVI;MAWJ,WAXI;MAYJ,cAZI;MAaJ,aAbI;MAcJ;IAdI,IAgBF,YAAY,CAAC,qBAAb,CACI,CAAC,CAAC,KADN,EAEI,MAAM,CAAC,KAFX,EAE8C,OAF9C,EAEuD,GAFvD,EAGI;IAAO;IAHX,EAG6B,SAH7B,CAhBJ;IAqBA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,KAAY,QAAQ,CAAC,MADzB,EAEI,MAAM,YAAY,uBAAuB,OAAnC,GACF,qCAAqC,QAAQ,CAAC,MAAM,YADlD,GAEF,GAAG,EAAE,CAAC,IAAI,EAJlB;IAMA,MAAM,GAAG,GACL,IAAI,CAAC,aAAL,CACI,QADJ,EACc,UAAU,CAAC,IAAX,CAAgB,GAAhB,CAAoB,EAAE,CAAC,MAAvB,EAA+B,MAD7C,CADJ,CA3CuC,CAgDvC;IACA;;IACA,MAAM,SAAS,GACX,IAAI,CAAC,yBAAL,CAA+B,CAAC,CAAC,KAAjC,EAAwC,CAAC,CAAC,KAA1C,CADJ,CAlDuC,CAqDvC;IACA;IACA;IACA;IACA;;IACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,SAApB,EAA+B,EAAE,CAAjC,EAAoC;MAClC,KAAK,IAAI,IAAI,GAAG,CAAhB,EAAmB,IAAI,GAAG,SAA1B,EAAqC,EAAE,IAAvC,EAA6C;QAC3C,MAAM,IAAI,GAAG,IAAI,GAAG,YAAP,GAAsB,OAAO,CAAC,GAA3C;;QACA,KAAK,IAAI,IAAI,GAAG,CAAhB,EAAmB,IAAI,GAAG,QAA1B,EAAoC,EAAE,IAAtC,EAA4C;UAC1C,MAAM,IAAI,GAAG,IAAI,GAAG,WAAP,GAAqB,OAAO,CAAC,IAA1C;;UACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,UAApB,EAAgC,EAAE,CAAlC,EAAqC;YACnC,IAAI,MAAM,GAAG,MAAM,CAAC,gBAApB;YACA,IAAI,MAAM,GAAI,IAAI,GAAG,CAAR,GAAa,CAAb,GAAiB,IAA9B;YACA,IAAI,MAAM,GAAI,IAAI,GAAG,CAAR,GAAa,CAAb,GAAiB,IAA9B;;YACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,YAApB,EAAkC,EAAE,CAApC,EAAuC;cACrC,MAAM,GAAG,GAAG,IAAI,GAAG,CAAC,GAAG,cAAvB;;cACA,IAAI,GAAG,IAAI,CAAP,IAAY,GAAG,GAAG,QAAtB,EAAgC;gBAC9B,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,WAApB,EAAiC,EAAE,CAAnC,EAAsC;kBACpC,MAAM,GAAG,GAAG,IAAI,GAAG,CAAC,GAAG,aAAvB;;kBACA,IAAI,GAAG,IAAI,CAAP,IAAY,GAAG,GAAG,OAAtB,EAA+B;oBAC7B,MAAM,GAAG,GAAG,EAAE,CAAC,CAAD,CAAF,CAAM,GAAN,EAAW,GAAX,EAAgB,CAAhB,IAAqB,OAAO,CAAC,CAAD,CAAP,CAAW,CAAX,EAAc,CAAd,CAAjC;;oBACA,IAAI,GAAG,GAAG,MAAV,EAAkB;sBAChB,MAAM,GAAG,GAAT;sBACA,MAAM,GAAG,GAAT;sBACA,MAAM,GAAG,GAAT;oBACD;kBACF;gBACF;cACF;YACF;;YACD,SAAS,CAAC,CAAD,CAAT,CAAa,MAAb,EAAqB,MAArB,EAA6B,CAA7B,KAAmC,GAAG,CAAC,CAAD,CAAH,CAAO,IAAP,EAAa,IAAb,EAAmB,CAAnB,CAAnC;UACD;QACF;MACF;IACF;;IAED,MAAM,MAAM,GAAG,UAAU,CAAC,KAAX,CACX,IAAI,CAAC,YAAL,CAAkB,SAAlB,EAA6B,CAAC,CAAC,KAA/B,CADW,EAC4B,CAAC,CAAC,KAD9B,EACqC,CAAC,CAAC,KADvC,CAAf;IAGA,OAAO;MAAC,MAAD;MAAS,KAAK,EAAE,CAAC,CAAC,KAAlB;MAAyB,KAAK,EAAE,CAAC,CAAC;IAAlC,CAAP;EACD;AAhGwD,CAApD","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Dilation2DAttrs, Dilation2DBackpropInput, Tensor3D, Tensor4D, TypedArray, util} from '@tensorflow/tfjs-core';\nimport {KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const dilation2DBackpropInputConfig: KernelConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend, attrs}) => {\n    const {x, filter, dy} =\n        inputs as {x: Tensor4D, filter: Tensor3D, dy: Tensor4D};\n    const {strides, pad, dilations} = attrs as {} as Dilation2DAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const $x =\n        util.toNestedArray(\n            x.shape, cpuBackend.data.get(x.dataId).values as TypedArray) as\n        number[][][][];\n\n    const $filter = util.toNestedArray(\n                        filter.shape,\n                        cpuBackend.data.get(filter.dataId).values as\n                            TypedArray) as number[][][];\n\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } =\n        backend_util.computeDilation2DInfo(\n            x.shape as [number, number, number, number],\n            filter.shape as [number, number, number], strides, pad,\n            'NHWC' /* dataFormat */, dilations);\n\n    util.assert(\n        dy.rank === outShape.length,\n        () => `Error in ${Dilation2DBackpropInput}, dy ` +\n            `must have the same rank as output ${outShape.length}, but got ` +\n            `${dy.rank}`);\n\n    const $dy =\n        util.toNestedArray(\n            outShape, cpuBackend.data.get(dy.dataId).values as TypedArray) as\n        number[][][][];\n\n    // The computed gradients has the same dimensions as the input:\n    // [batch, inputHeight, inputCols, inChannel]\n    const gradients =\n        util.makeZerosNestedTypedArray(x.shape, x.dtype) as number[][][][];\n\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hInMax = (hBeg < 0) ? 0 : hBeg;\n            let wInMax = (wBeg < 0) ? 0 : wBeg;\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n                    if (val > curVal) {\n                      curVal = val;\n                      hInMax = hIn;\n                      wInMax = wIn;\n                    }\n                  }\n                }\n              }\n            }\n            gradients[b][hInMax][wInMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(\n        util.toTypedArray(gradients, x.dtype), x.shape, x.dtype);\n\n    return {dataId, shape: x.shape, dtype: x.dtype};\n  }\n};\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}