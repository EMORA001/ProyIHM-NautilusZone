{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape'; // Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\n\nfunction getShapeForBatchMatMul(shape, isChannelsLast) {\n  const length = shape.length;\n\n  if (length >= 3) {\n    return isChannelsLast ? [...shape.slice(0, -3)\n    /* batch */\n    , shape[length - 3] * shape[length - 2]\n    /* height * width */\n    , shape[length - 1]\n    /* channel */\n    ] : [...shape.slice(0, -3)\n    /* batch */\n    , shape[length - 3]\n    /* channel */\n    , shape[length - 2] * shape[length - 1]\n    /* height * width */\n    ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n} // For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\n\n\nexport function conv2dByMatMul(_ref) {\n  let {\n    x,\n    filter,\n    convInfo,\n    backend,\n    bias = null,\n    preluActivationWeights = null,\n    leakyreluAlpha = 0,\n    activation = null\n  } = _ref;\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  } // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n\n\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD; // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked && isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 && util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed'); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  } else {\n    const numCols = convInfo.outHeight * convInfo.outWidth;\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: isChannelsLast ? [convInfo.batchSize, numCols, convInfo.inChannels] : [convInfo.batchSize, convInfo.inChannels, numCols]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: isChannelsLast ? xReshaped : filterReshaped,\n      b: isChannelsLast ? filterReshaped : xReshaped,\n      transposeA: !isChannelsLast,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n} // Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\n\nexport function conv2dWithIm2Row(_ref2) {\n  let {\n    x,\n    filter,\n    convInfo,\n    backend,\n    bias = null,\n    preluActivationWeights = null,\n    leakyreluAlpha = 0,\n    activation = null\n  } = _ref2;\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [x.shape, [convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels], [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]];\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: x2ColShape\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(isChannelsLast ? im2ColReshaped.shape : w2Row.shape, isChannelsLast ? w2Row.shape : im2ColReshaped.shape, isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] : [convInfo.batchSize, convInfo.outChannels, numCols], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n\n  if (bias) {\n    inputs.push(bias);\n  }\n\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(product);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}","map":{"version":3,"sources":["../../../../../../tfjs-backend-webgl/src/kernels/Conv2D_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAkC,IAAlC,QAA6C,uBAA7C;AAKA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,SAAQ,4BAAR,QAA2C,oCAA3C;AACA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,OAAO,KAAK,UAAZ,MAA4B,eAA5B;AAEA,SAAQ,eAAR,EAAyB,2BAAzB,QAA2D,oBAA3D;AACA,SAAQ,QAAR,QAAuB,YAAvB;AACA,SAAQ,OAAR,QAAsB,WAAtB,C,CAaA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAAS,sBAAT,CACI,KADJ,EACqB,cADrB,EAC4C;EAC1C,MAAM,MAAM,GAAG,KAAK,CAAC,MAArB;;EACA,IAAI,MAAM,IAAI,CAAd,EAAiB;IACf,OAAO,cAAc,GACjB,CACE,GAAG,KAAK,CAAC,KAAN,CAAY,CAAZ,EAAe,CAAC,CAAhB;IAAmB;IADxB,EAEE,KAAK,CAAC,MAAM,GAAG,CAAV,CAAL,GAAoB,KAAK,CAAC,MAAM,GAAG,CAAV;IAAa;IAFxC,EAGE,KAAK,CAAC,MAAM,GAAG,CAAV;IAAa;IAHpB,CADiB,GAMjB,CACE,GAAG,KAAK,CAAC,KAAN,CAAY,CAAZ,EAAe,CAAC,CAAhB;IAAmB;IADxB,EACqC,KAAK,CAAC,MAAM,GAAG,CAAV;IAAa;IADvD,EAEE,KAAK,CAAC,MAAM,GAAG,CAAV,CAAL,GAAoB,KAAK,CAAC,MAAM,GAAG,CAAV;IAAa;IAFxC,CANJ;EAUD,CAXD,MAWO,IAAI,CAAC,cAAD,IAAmB,MAAM,KAAK,CAA9B,IAAmC,KAAK,CAAC,CAAD,CAAL,GAAW,CAAlD,EAAqD;IAC1D,OAAO,CAAC,KAAK,CAAC,CAAD,CAAN,EAAW,CAAX,CAAP;EACD,CAFM,MAEA;IACL,OAAO,IAAP;EACD;AACF,C,CAED;AACA;AACA;;;AACA,OAAM,SAAU,cAAV,OASS;EAAA,IATgB;IAC7B,CAD6B;IAE7B,MAF6B;IAG7B,QAH6B;IAI7B,OAJ6B;IAK7B,IAAI,GAAG,IALsB;IAM7B,sBAAsB,GAAG,IANI;IAO7B,cAAc,GAAG,CAPY;IAQ7B,UAAU,GAAG;EARgB,CAShB;EACb;EACA;EACA,MAAM,MAAM,GAAG,CAAC,CAAC,KAAjB;EACA,MAAM,QAAQ,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,CAAC,CAAC,MAAtB,CAAjB;EACA,MAAM,eAAe,GAAG,QAAQ,CAAC,UAAjC;EACA,MAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CAAlD;EACA,MAAM,gBAAgB,GAAG,QAAQ,CAAC,WAAlC;EACA,MAAM,cAAc,GAAG,QAAQ,CAAC,UAAT,KAAwB,cAA/C;EACA,MAAM,UAAU,GAAG,KAAnB;EACA,MAAM,UAAU,GAAG,KAAnB;EAEA,IAAI,GAAJ;EACA,MAAM,aAAa,GAAiB,EAApC;;EAEA,IAAI,sBAAsB,IAAI,IAA9B,EAAoC;IAClC,MAAM,WAAW,GACb,sBAAsB,CAAC,sBAAsB,CAAC,KAAxB,EAA+B,cAA/B,CAD1B;;IAEA,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,sBAAsB,GAAG,OAAO,CAAC;QAC/B,MAAM,EAAE;UAAC,CAAC,EAAE;QAAJ,CADuB;QAE/B,OAF+B;QAG/B,KAAK,EAAE;UAAC,KAAK,EAAE;QAAR;MAHwB,CAAD,CAAhC;MAKA,aAAa,CAAC,IAAd,CAAmB,sBAAnB;IACD;EACF;;EAED,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,MAAM,WAAW,GAAG,sBAAsB,CAAC,IAAI,CAAC,KAAN,EAAa,cAAb,CAA1C;;IACA,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,IAAI,GAAG,OAAO,CAAC;QAAC,MAAM,EAAE;UAAC,CAAC,EAAE;QAAJ,CAAT;QAAoB,OAApB;QAA6B,KAAK,EAAE;UAAC,KAAK,EAAE;QAAR;MAApC,CAAD,CAAd;MACA,aAAa,CAAC,IAAd,CAAmB,IAAnB;IACD;EACF,CAlCY,CAoCb;EACA;;;EACA,MAAM,yBAAyB,GAC3B,CAAC,WAAW,KAAK,CAAhB,IAAqB,gBAAgB,KAAK,CAA3C,KACA,eAAe,GAAG,2BAFtB,CAtCa,CA0Cb;EACA;EACA;EACA;;EACA,MAAM,WAAW,GAAG,CAAC,yBAAD,IAA8B,QAAQ,CAAC,QAAvC,IAChB,cADgB,IACE,QAAQ,CAAC,OAAT,IAAoB,IADtB,IAC8B,MAAM,CAAC,CAAD,CAAN,GAAY,CAAZ,KAAkB,CADhD,IAEhB,IAAI,CAAC,WAAL,CAAiB,QAAQ,CAAC,KAAT,CAAe,KAAf,CAAqB,CAAC,CAAtB,CAAjB,EAA2C,MAAM,CAAC,KAAP,CAAa,CAAC,CAAd,CAA3C,CAFJ;;EAIA,IAAI,WAAJ,EAAiB;IACf;IACA;IACA;IACA;IACA;IACA;IACA,MAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,IAAyB,MAAM,CAAC,CAAD,CAAN,GAAY,CAArC,CAApB;IACA,MAAM,SAAS,GAAe;MAC5B,MAAM,EAAE,CAAC,CAAC,MADkB;MAE5B,KAAK,EAAE,CAAC,CAAD,EAAI,WAAJ,EAAiB,QAAQ,CAAC,UAA1B,CAFqB;MAG5B,KAAK,EAAE,CAAC,CAAC;IAHmB,CAA9B,CARe,CAaf;IACA;IACA;IACA;IACA;IACA;IACA;IACA;;IACA,MAAM,qBAAqB,GAAG,QAAQ,CAAC,KAAvC;IACA,QAAQ,CAAC,KAAT,GAAiB,QAAQ,CAAC,KAAT,CAAe,KAAf,EAAjB;IACA,QAAQ,CAAC,KAAT,CAAe,QAAQ,CAAC,KAAT,CAAe,MAAf,GAAwB,CAAvC;IACA,IAAI,CAAC,MAAL,CACI,UAAU,CAAC,aAAX,CAAyB,QAAQ,CAAC,KAAlC,EAAyC,SAAS,CAAC,KAAnD,CADJ,EAEI,MAAM,kBAAkB,QAAQ,CAAC,KAAK,OAClC,SAAS,CAAC,KAAK,aAHvB;IAIA,MAAM,cAAc,GAAG,OAAO,CAAC;MAC7B,MAAM,EAAE;QAAC,CAAC,EAAE;MAAJ,CADqB;MAE7B,OAF6B;MAG7B,KAAK,EAAE;QAAC,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;MAAR;IAHsB,CAAD,CAA9B;IAKA,aAAa,CAAC,IAAd,CAAmB,cAAnB;IACA,MAAM,aAAa,GAAG,eAAe,CAAC;MACpC,CAAC,EAAE,SADiC;MAEpC,CAAC,EAAE,cAFiC;MAGpC,OAHoC;MAIpC,UAJoC;MAKpC,UALoC;MAMpC,IANoC;MAOpC,UAPoC;MAQpC,sBARoC;MASpC;IAToC,CAAD,CAArC;IAYA,MAAM,oBAAoB,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,aAAa,CAAC,MAAlC,CAA7B;IACA,IAAI,CAAC,MAAL,CACI,oBAAoB,CAAC,QADzB,EAEI,MAAM,6CAFV,EA/Ce,CAkDf;;IACA,QAAQ,CAAC,KAAT,GAAiB,qBAAjB,CAnDe,CAoDf;IACA;;IACA,oBAAoB,CAAC,KAArB,GAA6B,QAAQ,CAAC,QAAtC;IAEA,GAAG,GAAG,QAAQ,CAAC;MAAC,MAAM,EAAE;QAAC,CAAC,EAAE;MAAJ,CAAT;MAA6B;IAA7B,CAAD,CAAd;IACA,GAAG,CAAC,KAAJ,GAAY,QAAQ,CAAC,QAArB;IAEA,aAAa,CAAC,IAAd,CAAmB,aAAnB;EACD,CA5DD,MA4DO;IACL,MAAM,OAAO,GAAG,QAAQ,CAAC,SAAT,GAAqB,QAAQ,CAAC,QAA9C;IACA,MAAM,SAAS,GAAG,OAAO,CAAC;MACxB,MAAM,EAAE;QAAC;MAAD,CADgB;MAExB,OAFwB;MAGxB,KAAK,EAAE;QACL,KAAK,EAAE,cAAc,GACjB,CAAC,QAAQ,CAAC,SAAV,EAAqB,OAArB,EAA8B,QAAQ,CAAC,UAAvC,CADiB,GAEjB,CAAC,QAAQ,CAAC,SAAV,EAAqB,QAAQ,CAAC,UAA9B,EAA0C,OAA1C;MAHC;IAHiB,CAAD,CAAzB;IASA,MAAM,cAAc,GAAG,OAAO,CAAC;MAC7B,MAAM,EAAE;QAAC,CAAC,EAAE;MAAJ,CADqB;MAE7B,OAF6B;MAG7B,KAAK,EAAE;QAAC,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;MAAR;IAHsB,CAAD,CAA9B;IAKA,MAAM,MAAM,GAAG,eAAe,CAAC;MAC7B,CAAC,EAAE,cAAc,GAAG,SAAH,GAAe,cADH;MAE7B,CAAC,EAAE,cAAc,GAAG,cAAH,GAAoB,SAFR;MAG7B,UAAU,EAAE,CAAC,cAHgB;MAI7B,UAJ6B;MAK7B,OAL6B;MAM7B,IAN6B;MAO7B,UAP6B;MAQ7B,sBAR6B;MAS7B;IAT6B,CAAD,CAA9B;IAYA,GAAG,GAAG,OAAO,CACT;MAAC,MAAM,EAAE;QAAC,CAAC,EAAE;MAAJ,CAAT;MAAsB,OAAtB;MAA+B,KAAK,EAAE;QAAC,KAAK,EAAE,QAAQ,CAAC;MAAjB;IAAtC,CADS,CAAb;IAGA,aAAa,CAAC,IAAd,CAAmB,SAAnB;IACA,aAAa,CAAC,IAAd,CAAmB,cAAnB;IACA,aAAa,CAAC,IAAd,CAAmB,MAAnB;EACD;;EAED,KAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;IAC7B,OAAO,CAAC,6BAAR,CAAsC,CAAtC;EACD;;EAED,OAAO,GAAP;AACD,C,CAED;AACA;;AACA,OAAM,SAAU,gBAAV,QASS;EAAA,IATkB;IAC/B,CAD+B;IAE/B,MAF+B;IAG/B,QAH+B;IAI/B,OAJ+B;IAK/B,IAAI,GAAG,IALwB;IAM/B,sBAAsB,GAAG,IANM;IAO/B,cAAc,GAAG,CAPc;IAQ/B,UAAU,GAAG;EARkB,CASlB;EACb;EACA;EACA;EACA;EACA;EACA;EACA,MAAM;IACJ,WADI;IAEJ,YAFI;IAGJ,UAHI;IAIJ,QAJI;IAKJ,SALI;IAMJ;EANI,IAOF,QAPJ;EASA,MAAM,cAAc,GAAG,UAAU,KAAK,cAAtC;EAEA,MAAM,SAAS,GAAG,WAAW,GAAG,YAAd,GAA6B,UAA/C;EACA,MAAM,OAAO,GAAG,SAAS,GAAG,QAA5B;EACA,MAAM,UAAU,GAAG,CAAC,QAAQ,CAAC,SAAV,EAAqB,SAArB,EAAgC,OAAhC,CAAnB;EACA,MAAM,UAAU,GAAG,IAAnB;EACA,MAAM,UAAU,GAAG,KAAnB;EAEA,MAAM,aAAa,GAAiB,EAApC;;EAEA,IAAI,sBAAsB,IAAI,IAA9B,EAAoC;IAClC,MAAM,WAAW,GACb,sBAAsB,CAAC,sBAAsB,CAAC,KAAxB,EAA+B,cAA/B,CAD1B;;IAEA,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,sBAAsB,GAAG,OAAO,CAAC;QAC/B,MAAM,EAAE;UAAC,CAAC,EAAE;QAAJ,CADuB;QAE/B,OAF+B;QAG/B,KAAK,EAAE;UAAC,KAAK,EAAE;QAAR;MAHwB,CAAD,CAAhC;MAKA,aAAa,CAAC,IAAd,CAAmB,sBAAnB;IACD;EACF;;EAED,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,MAAM,WAAW,GAAG,sBAAsB,CAAC,IAAI,CAAC,KAAN,EAAa,cAAb,CAA1C;;IACA,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,IAAI,GAAG,OAAO,CAAC;QAAC,MAAM,EAAE;UAAC,CAAC,EAAE;QAAJ,CAAT;QAAoB,OAApB;QAA6B,KAAK,EAAE;UAAC,KAAK,EAAE;QAAR;MAApC,CAAD,CAAd;MACA,aAAa,CAAC,IAAd,CAAmB,IAAnB;IACD;EACF;;EAED,MAAM,KAAK,GAAG,OAAO,CAAC;IACpB,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CADY;IAEpB,OAFoB;IAGpB,KAAK,EAAE;MAAC,KAAK,EAAE,CAAC,CAAD,EAAI,SAAJ,EAAe,IAAI,CAAC,aAAL,CAAmB,MAAM,CAAC,KAA1B,IAAmC,SAAlD;IAAR;EAHa,CAAD,CAArB;EAKA,aAAa,CAAC,IAAd,CAAmB,KAAnB;EAEA,MAAM,aAAa,GAAG,IAAI,mBAAJ,CAAwB,UAAxB,EAAoC,QAApC,CAAtB;EACA,MAAM,YAAY,GAAG,CACnB,CAAC,CAAC,KADiB,EACV,CAAC,QAAQ,CAAC,OAAT,CAAiB,GAAlB,EAAuB,QAAQ,CAAC,OAAT,CAAiB,IAAxC,CADU,EAEnB,CAAC,QAAQ,CAAC,YAAV,EAAwB,QAAQ,CAAC,WAAjC,CAFmB,EAGnB,CAAC,QAAQ,CAAC,cAAV,EAA0B,QAAQ,CAAC,aAAnC,CAHmB,EAGgC,CAAC,QAAQ,CAAC,UAAV,CAHhC,EAInB,CAAC,QAAQ,CAAC,WAAT,GAAuB,QAAQ,CAAC,UAAjC,CAJmB,EAI2B,CAAC,QAAQ,CAAC,QAAV,CAJ3B,CAArB;EAMA,MAAM,MAAM,GACR,OAAO,CAAC,eAAR,CAAwB,aAAxB,EAAuC,CAAC,CAAD,CAAvC,EAA4C,SAA5C,EAAuD,YAAvD,CADJ;EAEA,MAAM,cAAc,GAChB,OAAO,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAsB,OAAtB;IAA+B,KAAK,EAAE;MAAC,KAAK,EAAE;IAAR;EAAtC,CAAD,CADX;EAGA,aAAa,CAAC,IAAd,CAAmB,MAAnB;EACA,aAAa,CAAC,IAAd,CAAmB,cAAnB;EAEA,MAAM,OAAO,GAAG,IAAI,IAAI,IAAxB;EACA,MAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAA5D;EACA,MAAM,iBAAiB,GAAG,UAAU,KAAK,WAAzC;EACA,MAAM,eAAe,GACjB,UAAU,GAAG,4BAA4B,CAAC,UAAD,EAAa,IAAb,CAA/B,GAAoD,IADlE;EAEA,MAAM,aAAa,GAAG,IAAI,mBAAJ,CAClB,cAAc,GAAG,cAAc,CAAC,KAAlB,GACG,KAAK,CAAC,KAFL,EAGlB,cAAc,GAAG,KAAK,CAAC,KAAT,GACG,cAAc,CAAC,KAJd,EAKlB,cAAc,GAAG,CAAC,QAAQ,CAAC,SAAV,EAAqB,OAArB,EAA8B,QAAQ,CAAC,WAAvC,CAAH,GACG,CAAC,QAAQ,CAAC,SAAV,EAAqB,QAAQ,CAAC,WAA9B,EAA2C,OAA3C,CANC,EAOlB,UAPkB,EAON,UAPM,EAOM,OAPN,EAOe,eAPf,EAQlB,yBARkB,EAQS,iBART,CAAtB;EASA,MAAM,MAAM,GACR,cAAc,GAAG,CAAC,cAAD,EAAiB,KAAjB,CAAH,GAA6B,CAAC,KAAD,EAAQ,cAAR,CAD/C;;EAEA,IAAI,IAAJ,EAAU;IACR,MAAM,CAAC,IAAP,CAAY,IAAZ;EACD;;EACD,IAAI,yBAAJ,EAA+B;IAC7B,MAAM,CAAC,IAAP,CAAY,sBAAZ;EACD;;EACD,IAAI,iBAAJ,EAAuB;IACrB,MAAM,eAAe,GAAG,OAAO,CAAC,cAAR,CACpB,EADoB,EAChB,SADgB,EAEpB,IAAI,CAAC,iBAAL,CAAuB,cAAvB,EAA0D,SAA1D,CAFoB,CAAxB;IAGA,MAAM,CAAC,IAAP,CAAY,eAAZ;IACA,aAAa,CAAC,IAAd,CAAmB,eAAnB;EACD;;EACD,MAAM,OAAO,GAAG,OAAO,CAAC,eAAR,CAAwB,aAAxB,EAAuC,MAAvC,EAA+C,SAA/C,CAAhB;EACA,MAAM,GAAG,GAAG,OAAO,CACf;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAuB,OAAvB;IAAgC,KAAK,EAAE;MAAC,KAAK,EAAE,QAAQ,CAAC;IAAjB;EAAvC,CADe,CAAnB;EAGA,aAAa,CAAC,IAAd,CAAmB,OAAnB;;EACA,KAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;IAC7B,OAAO,CAAC,6BAAR,CAAsC,CAAtC;EACD;;EAED,OAAO,GAAP;AACD","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\n\n// import {assertAndGetBroadcastShape} from\n// '../../../tfjs-core/src/ops/broadcast_util';\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {Im2ColPackedProgram} from '../im2col_packed_gpu';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\n\nimport {batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD} from './BatchMatMul_impl';\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\nfunction getShapeForBatchMatMul(\n    shape: number[], isChannelsLast: boolean): number[] {\n  const length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ?\n        [\n          ...shape.slice(0, -3) /* batch */,\n          shape[length - 3] * shape[length - 2] /* height * width */,\n          shape[length - 1] /* channel */\n        ] :\n        [\n          ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,\n          shape[length - 2] * shape[length - 1] /* height * width */\n        ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n\n  let out: TensorInfo;\n  const intermediates: TensorInfo[] = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked =\n      (outerShapeX === 1 || outerShapeFilter === 1) &&\n      sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n\n  // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked &&\n      isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 &&\n      util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped: TensorInfo = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(\n        webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape),\n        () => `packed reshape ${xTexData.shape} to ${\n            xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(\n        pointwiseConvTexData.isPacked,\n        () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n\n    out = identity({inputs: {x: pointwiseConv}, backend});\n    out.shape = convInfo.outShape;\n\n    intermediates.push(pointwiseConv);\n  } else {\n    const numCols = convInfo.outHeight * convInfo.outWidth;\n    const xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {\n        shape: isChannelsLast ?\n            [convInfo.batchSize, numCols, convInfo.inChannels] :\n            [convInfo.batchSize, convInfo.inChannels, numCols]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    const result = batchMatMulImpl({\n      a: isChannelsLast ? xReshaped : filterReshaped,\n      b: isChannelsLast ? filterReshaped : xReshaped,\n      transposeA: !isChannelsLast,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    out = reshape(\n        {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n\n  const isChannelsLast = dataFormat === 'channelsLast';\n\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n\n  const intermediates: TensorInfo[] = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  const w2Row = reshape({\n    inputs: {x: filter},\n    backend,\n    attrs: {shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]}\n  });\n  intermediates.push(w2Row);\n\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [\n    x.shape, [convInfo.padInfo.top, convInfo.padInfo.left],\n    [convInfo.strideHeight, convInfo.strideWidth],\n    [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels],\n    [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]\n  ];\n  const im2Col =\n      backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n  const im2ColReshaped =\n      reshape({inputs: {x: im2Col}, backend, attrs: {shape: x2ColShape}});\n\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation =\n      activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(\n      isChannelsLast ? im2ColReshaped.shape as [number, number, number] :\n                       w2Row.shape as [number, number, number],\n      isChannelsLast ? w2Row.shape as [number, number, number] :\n                       im2ColReshaped.shape as [number, number, number],\n      isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] :\n                       [convInfo.batchSize, convInfo.outChannels, numCols],\n      transposeA, transposeB, hasBias, fusedActivation,\n      hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs: TensorInfo[] =\n      isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo(\n        [], 'float32',\n        util.createScalarValue(leakyreluAlpha as {} as 'float32', 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const out = reshape(\n      {inputs: {x: product}, backend, attrs: {shape: convInfo.outShape}});\n\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}