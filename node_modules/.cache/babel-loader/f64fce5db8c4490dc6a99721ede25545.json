{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class RMSPropOptimizer extends Optimizer {\n  constructor(learningRate) {\n    let decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n    let momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n    let epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    let centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n    super();\n    this.learningRate = learningRate;\n    this.decay = decay;\n    this.momentum = momentum;\n    this.epsilon = epsilon;\n    this.accumulatedMeanSquares = [];\n    this.accumulatedMoments = [];\n    this.accumulatedMeanGrads = [];\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable; // Centered gradient\n\n          const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n          const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n\n    if (this.centered) {\n      this.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    }\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n  }\n\n}\n/** @nocollapse */\n\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\n\nregisterClass(RMSPropOptimizer);","map":{"version":3,"sources":["../../../../../../tfjs-core/src/optimizers/rmsprop_optimizer.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,OAAR,EAAiB,IAAjB,QAA4B,YAA5B;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,IAAR,QAAmB,aAAnB;AACA,SAAQ,MAAR,QAAqB,eAArB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,SAAR,QAAwB,mBAAxB;AACA,SAAoB,aAApB,QAA+E,kBAA/E;AAGA,SAAQ,SAAR,QAA2C,aAA3C;AAEA;;AACA,OAAM,MAAO,gBAAP,SAAgC,SAAhC,CAAyC;EAS7C,WAAA,CACc,YADd,EAGoB;IAAA,IAF0B,KAE1B,uEAFkC,GAElC;IAAA,IADN,QACM,uEADK,GACL;IAAA,IADoB,OACpB,uEADsC,IACtC;IAAA,IAAhB,QAAgB,uEAAL,KAAK;IAClB;IAHY,KAAA,YAAA,GAAA,YAAA;IAAgC,KAAA,KAAA,GAAA,KAAA;IAChC,KAAA,QAAA,GAAA,QAAA;IAA0B,KAAA,OAAA,GAAA,OAAA;IANhC,KAAA,sBAAA,GAA8C,EAA9C;IACA,KAAA,kBAAA,GAA0C,EAA1C;IACA,KAAA,oBAAA,GAA4C,EAA5C;IAQN,KAAK,QAAL,GAAgB,QAAhB;;IAEA,IAAI,OAAO,IAAI,IAAf,EAAqB;MACnB,KAAK,OAAL,GAAe,MAAM,CAAC,OAAP,CAAe,OAAf,EAAf;IACD;;IACD,IAAI,YAAY,IAAI,IAApB,EAA0B;MACxB,MAAM,IAAI,KAAJ,CAAU,oDAAV,CAAN;IACD;EACF;;EAED,cAAc,CAAC,iBAAD,EAAgD;IAC5D,MAAM,aAAa,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IAClB,iBAAiB,CAAC,GAAlB,CAAsB,IAAI,IAAI,IAAI,CAAC,IAAnC,CADkB,GAElB,MAAM,CAAC,IAAP,CAAY,iBAAZ,CAFJ;IAIA,aAAa,CAAC,OAAd,CAAsB,CAAC,IAAD,EAAO,CAAP,KAAY;MAChC,MAAM,KAAK,GAAG,MAAM,CAAC,mBAAP,CAA2B,IAA3B,CAAd;MACA,MAAM,SAAS,GAAG,KAAlB;;MACA,IAAI,KAAK,sBAAL,CAA4B,CAA5B,KAAkC,IAAtC,EAA4C;QAC1C,KAAK,sBAAL,CAA4B,CAA5B,IAAiC;UAC/B,YAAY,EAAE,GAAG,IAAI,MADU;UAE/B,QAAQ,EAAE,IAAI,CAAC,MAAM,SAAS,CAAC,KAAD,CAAT,CAAiB,QAAjB,CAA0B,SAA1B,CAAP;QAFiB,CAAjC;MAID;;MACD,IAAI,KAAK,kBAAL,CAAwB,CAAxB,KAA8B,IAAlC,EAAwC;QACtC,KAAK,kBAAL,CAAwB,CAAxB,IAA6B;UAC3B,YAAY,EAAE,GAAG,IAAI,WADM;UAE3B,QAAQ,EAAE,IAAI,CAAC,MAAM,SAAS,CAAC,KAAD,CAAT,CAAiB,QAAjB,CAA0B,SAA1B,CAAP;QAFa,CAA7B;MAID;;MACD,IAAI,KAAK,oBAAL,CAA0B,CAA1B,KAAgC,IAAhC,IAAwC,KAAK,QAAjD,EAA2D;QACzD,KAAK,oBAAL,CAA0B,CAA1B,IAA+B;UAC7B,YAAY,EAAE,GAAG,IAAI,KADQ;UAE7B,QAAQ,EAAE,IAAI,CAAC,MAAM,SAAS,CAAC,KAAD,CAAT,CAAiB,QAAjB,CAA0B,SAA1B,CAAP;QAFe,CAA/B;MAID;;MAED,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,CAAD,CAAjB,CAAqB,MADR,GAEb,iBAAiB,CAAC,IAAD,CAFrB;;MAGA,IAAI,QAAQ,IAAI,IAAhB,EAAsB;QACpB;MACD;;MAED,MAAM,qBAAqB,GAAG,KAAK,sBAAL,CAA4B,CAA5B,EAA+B,QAA7D;MACA,MAAM,kBAAkB,GAAG,KAAK,kBAAL,CAAwB,CAAxB,EAA2B,QAAtD;MACA,IAAI,CAAC,MAAK;QACR,MAAM,wBAAwB,GAC1B,GAAG,CAAC,GAAG,CAAC,qBAAD,EAAwB,KAAK,KAA7B,CAAJ,EACC,GAAG,CAAC,MAAM,CAAC,QAAD,CAAP,EAAmB,IAAI,KAAK,KAA5B,CADJ,CADP;;QAIA,IAAI,KAAK,QAAT,EAAmB;UACjB,MAAM,mBAAmB,GAAG,KAAK,oBAAL,CAA0B,CAA1B,EAA6B,QAAzD,CADiB,CAEjB;;UACA,MAAM,sBAAsB,GACxB,GAAG,CAAC,GAAG,CAAC,mBAAD,EAAsB,KAAK,KAA3B,CAAJ,EACC,GAAG,CAAC,QAAD,EAAW,IAAI,KAAK,KAApB,CADJ,CADP;UAIA,MAAM,gBAAgB,GAClB,GAAG,CAAC,GAAG,CAAC,QAAD,EAAW,KAAK,YAAhB,CAAJ,EACC,IAAI,CACA,GAAG,CAAC,wBAAD,EACC,GAAG,CAAC,MAAM,CAAC,sBAAD,CAAP,EAAiC,KAAK,OAAtC,CADJ,CADH,CADL,CADP;UAKA,MAAM,qBAAqB,GACvB,GAAG,CAAC,GAAG,CAAC,kBAAD,EAAqB,KAAK,QAA1B,CAAJ,EAAyC,gBAAzC,CADP;UAGA,qBAAqB,CAAC,MAAtB,CAA6B,wBAA7B;UACA,mBAAmB,CAAC,MAApB,CAA2B,sBAA3B;UACA,kBAAkB,CAAC,MAAnB,CAA0B,qBAA1B;UAEA,MAAM,QAAQ,GAAG,GAAG,CAAC,KAAD,EAAQ,qBAAR,CAApB;UACA,KAAK,CAAC,MAAN,CAAa,QAAb;QACD,CArBD,MAqBO;UACL;UACA,MAAM,wBAAwB,GAC1B,GAAG,CAAC,GAAG,CAAC,qBAAD,EAAwB,KAAK,KAA7B,CAAJ,EACC,GAAG,CAAC,MAAM,CAAC,QAAD,CAAP,EAAmB,IAAI,KAAK,KAA5B,CADJ,CADP;UAIA,MAAM,qBAAqB,GACvB,GAAG,CAAC,GAAG,CAAC,kBAAD,EAAqB,KAAK,QAA1B,CAAJ,EACC,GAAG,CAAC,GAAG,CAAC,QAAD,EAAW,KAAK,YAAhB,CAAJ,EACC,IAAI,CAAC,GAAG,CAAC,wBAAD,EAA2B,KAAK,OAAhC,CAAJ,CADL,CADJ,CADP;UAKA,qBAAqB,CAAC,MAAtB,CAA6B,wBAA7B;UACA,kBAAkB,CAAC,MAAnB,CAA0B,qBAA1B;UAEA,MAAM,QAAQ,GAAG,GAAG,CAAC,KAAD,EAAQ,qBAAR,CAApB;UACA,KAAK,CAAC,MAAN,CAAa,QAAb;QACD;MACF,CA3CG,CAAJ;IA4CD,CA3ED;IA4EA,KAAK,mBAAL;EACD;;EAED,OAAO,GAAA;IACL,IAAI,KAAK,sBAAL,IAA+B,IAAnC,EAAyC;MACvC,OAAO,CAAC,KAAK,sBAAL,CAA4B,GAA5B,CAAgC,CAAC,IAAI,CAAC,CAAC,QAAvC,CAAD,CAAP;IACD;;IACD,IAAI,KAAK,oBAAL,IAA6B,IAA7B,IAAqC,KAAK,QAA9C,EAAwD;MACtD,OAAO,CAAC,KAAK,oBAAL,CAA0B,GAA1B,CAA8B,CAAC,IAAI,CAAC,CAAC,QAArC,CAAD,CAAP;IACD;;IACD,IAAI,KAAK,kBAAL,IAA2B,IAA/B,EAAqC;MACnC,OAAO,CAAC,KAAK,kBAAL,CAAwB,GAAxB,CAA4B,CAAC,IAAI,CAAC,CAAC,QAAnC,CAAD,CAAP;IACD;EACF;;EAEe,MAAV,UAAU,GAAA;IACd;IACA,MAAM,SAAS,GACX,CAAC,GAAG,KAAK,sBAAT,EAAiC,GAAG,KAAK,kBAAzC,CADJ;;IAEA,IAAI,KAAK,QAAT,EAAmB;MACjB,SAAS,CAAC,IAAV,CAAe,GAAG,KAAK,oBAAvB;IACD;;IACD,OAAO,CAAC,MAAM,KAAK,cAAL,EAAP,EAA8B,MAA9B,CACH,SAAS,CAAC,GAAV,CAAc,CAAC,KAAK;MAAC,IAAI,EAAE,CAAC,CAAC,YAAT;MAAuB,MAAM,EAAE,CAAC,CAAC;IAAjC,CAAL,CAAf,CADG,CAAP;EAED;;EAEe,MAAV,UAAU,CAAC,YAAD,EAA4B;IAC1C,YAAY,GAAG,MAAM,KAAK,iBAAL,CAAuB,YAAvB,CAArB;IACA,MAAM,aAAa,GACf,KAAK,QAAL,GAAgB,YAAY,CAAC,MAAb,GAAsB,CAAtC,GAA0C,YAAY,CAAC,MAAb,GAAsB,CADpE;IAEA,MAAM,SAAS,GAAG,KAAlB;IACA,KAAK,sBAAL,GACI,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,aAAtB,EAAqC,GAArC,CAAyC,CAAC,KAAK;MACJ,YAAY,EAAE,CAAC,CAAC,IADZ;MAEJ,QAAQ,EAAE,CAAC,CAAC,MAAF,CAAS,QAAT,CACN,SADM;IAFN,CAAL,CAA1C,CADJ;IAMA,KAAK,kBAAL,GACI,YAAY,CAAC,KAAb,CAAmB,aAAnB,EAAkC,aAAa,GAAG,CAAlD,EACK,GADL,CACS,CAAC,KAAK;MACJ,YAAY,EAAE,CAAC,CAAC,IADZ;MAEJ,QAAQ,EAAE,CAAC,CAAC,MAAF,CAAS,QAAT,CAAkB,SAAlB;IAFN,CAAL,CADV,CADJ;;IAMA,IAAI,KAAK,QAAT,EAAmB;MACjB,KAAK,oBAAL,GACI,YAAY,CAAC,KAAb,CAAmB,aAAa,GAAG,CAAnC,EAAsC,aAAa,GAAG,CAAtD,EACK,GADL,CACS,CAAC,KAAK;QACJ,YAAY,EAAE,CAAC,CAAC,IADZ;QAEJ,QAAQ,EAAE,CAAC,CAAC,MAAF,CAAS,QAAT,CAAkB,SAAlB;MAFN,CAAL,CADV,CADJ;IAMD;EACF;;EAED,SAAS,GAAA;IACP,OAAO;MACL,gBAAgB,KAAK,YADhB;MAEL,SAAS,KAAK,KAFT;MAGL,YAAY,KAAK,QAHZ;MAIL,WAAW,KAAK,OAJX;MAKL,YAAY,KAAK;IALZ,CAAP;EAOD;EAED;;;EACiB,OAAV,UAAU,CACb,GADa,EACoB,MADpB,EACsC;IACrD,OAAO,IAAI,GAAJ,CACH,MAAM,CAAC,cAAD,CADH,EACqB,MAAM,CAAC,OAAD,CAD3B,EACsC,MAAM,CAAC,UAAD,CAD5C,EAEH,MAAM,CAAC,SAAD,CAFH,EAEgB,MAAM,CAAC,UAAD,CAFtB,CAAP;EAGD;;AA/K4C;AAC7C;;AACO,gBAAA,CAAA,SAAA,GAAY,SAAZ,C,CAAwB;;AA+KjC,aAAa,CAAC,gBAAD,CAAb","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'RMSProp';  // Note: Name matters for Python compatibility.\n  private centered: boolean;\n\n  private accumulatedMeanSquares: OptimizerVariable[] = [];\n  private accumulatedMoments: OptimizerVariable[] = [];\n  private accumulatedMeanGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected decay = 0.9,\n      protected momentum = 0.0, protected epsilon: number = null,\n      centered = false) {\n    super();\n\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare =\n            add(mul(accumulatedMeanSquare, this.decay),\n                mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad =\n              add(mul(accumulatedMeanGrad, this.decay),\n                  mul(gradient, 1 - this.decay));\n\n          const gradContribution =\n              div(mul(gradient, this.learningRate),\n                  sqrt(\n                      sub(newAccumulatedMeanSquare,\n                          add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum), gradContribution);\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare =\n              add(mul(accumulatedMeanSquare, this.decay),\n                  mul(square(gradient), 1 - this.decay));\n\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum),\n                  div(mul(gradient, this.learningRate),\n                      sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount =\n        this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedMoments =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n    if (this.centered) {\n      this.accumulatedMeanGrads =\n          weightValues.slice(variableCount * 2, variableCount * 3)\n              .map(v => ({\n                     originalName: v.name,\n                     variable: v.tensor.variable(trainable)\n                   }));\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['decay'], config['momentum'],\n        config['epsilon'], config['centered']);\n  }\n}\nregisterClass(RMSPropOptimizer);\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}