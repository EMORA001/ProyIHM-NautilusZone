{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor, util } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\n\nexport class GraphModel {\n  /**\n   * @param modelUrl url for the model, or an `io.IOHandler`.\n   * @param weightManifestUrl url for the weight file generated by\n   * scripts/convert.py script.\n   * @param requestOption options for Request, which allows to send credentials\n   * and custom headers.\n   * @param onProgress Optional, progress callback function, fired periodically\n   * before the load is completed.\n   */\n  constructor(modelUrl) {\n    let loadOptions = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    let tfio = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : io;\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n    this.io = tfio;\n\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n\n    this.resourceManager = new ResourceManager();\n  } // Returns the version information for the tensorflow model GraphDef.\n\n\n  get modelVersion() {\n    return this.version;\n  }\n\n  get inputNodes() {\n    return this.executor.inputNodes;\n  }\n\n  get outputNodes() {\n    return this.executor.outputNodes;\n  }\n\n  get inputs() {\n    return this.executor.inputs;\n  }\n\n  get outputs() {\n    return this.executor.outputs;\n  }\n\n  get weights() {\n    return this.executor.weightMap;\n  }\n\n  get metadata() {\n    return this.artifacts.userDefinedMetadata;\n  }\n\n  get modelSignature() {\n    return this.signature;\n  }\n\n  get modelStructuredOutputKeys() {\n    return this.structuredOutputKeys;\n  }\n\n  findIOHandler() {\n    const path = this.modelUrl;\n\n    if (path.load != null) {\n      // Path is an IO Handler.\n      this.handler = path;\n    } else if (this.loadOptions.requestInit != null) {\n      this.handler = this.io.browserHTTPRequest(path, this.loadOptions);\n    } else {\n      const handlers = this.io.getLoadHandlers(path, this.loadOptions);\n\n      if (handlers.length === 0) {\n        // For backward compatibility: if no load handler can be found,\n        // assume it is a relative http path.\n        handlers.push(this.io.browserHTTPRequest(path, this.loadOptions));\n      } else if (handlers.length > 1) {\n        throw new Error(`Found more than one (${handlers.length}) load handlers for ` + `URL '${[path]}'`);\n      }\n\n      this.handler = handlers[0];\n    }\n  }\n  /**\n   * Loads the model and weight files, construct the in memory weight map and\n   * compile the inference graph.\n   */\n\n\n  load() {\n    this.findIOHandler();\n\n    if (this.handler.load == null) {\n      throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n    }\n\n    const loadResult = this.handler.load();\n\n    if (util.isPromise(loadResult)) {\n      return loadResult.then(artifacts => this.loadSync(artifacts));\n    }\n\n    return this.loadSync(loadResult);\n  }\n  /**\n   * Synchronously construct the in memory weight map and\n   * compile the inference graph. Also initialize hashtable if any.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n\n\n  loadSync(artifacts) {\n    this.artifacts = artifacts;\n    const graph = this.artifacts.modelTopology;\n    let signature = this.artifacts.signature;\n\n    if (this.artifacts.userDefinedMetadata != null) {\n      const metadata = this.artifacts.userDefinedMetadata;\n\n      if (metadata.signature != null) {\n        signature = metadata.signature;\n      }\n\n      if (metadata.structuredOutputKeys != null) {\n        this.structuredOutputKeys = metadata.structuredOutputKeys;\n      }\n    }\n\n    this.signature = signature;\n    this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n    const weightMap = this.io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n    this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n    this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap); // Attach a model-level resourceManager to each executor to share resources,\n    // such as `HashTable`.\n\n    this.executor.resourceManager = this.resourceManager;\n\n    if (artifacts.modelInitializer != null && artifacts.modelInitializer.node != null) {\n      const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n      this.initializer = new GraphExecutor(initializer);\n      this.initializer.weightMap = this.executor.weightMap; // Attach a model-level resourceManager to the initializer, the\n      // hashTables created from when executing the initializer will be stored\n      // in the resourceManager.\n\n      this.initializer.resourceManager = this.resourceManager;\n      this.initializer.executeAsync({}, []);\n    }\n\n    return true;\n  }\n  /**\n   * Save the configuration and/or weights of the GraphModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const modelUrl =\n   *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n   * const model = await tf.loadGraphModel(modelUrl);\n   * const zeros = tf.zeros([1, 224, 224, 3]);\n   * model.predict(zeros).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * model.predict(zeros).print();\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n\n\n  async save(handlerOrURL, config) {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = this.io.getSaveHandlers(handlerOrURL);\n\n      if (handlers.length === 0) {\n        throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new Error(`Found more than one (${handlers.length}) save handlers for ` + `URL '${handlerOrURL}'`);\n      }\n\n      handlerOrURL = handlers[0];\n    }\n\n    if (handlerOrURL.save == null) {\n      throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n    }\n\n    return handlerOrURL.save(this.artifacts);\n  }\n  /**\n   * Execute the inference for the input tensors.\n   *\n   * @param input The input tensors, when there is single input for the model,\n   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n   * inputs params should be in either `tf.Tensor`[] if the input order is\n   * fixed, or otherwise NamedTensorMap format.\n   *\n   * For model with multiple inputs, we recommend you use NamedTensorMap as the\n   * input type, if you use `tf.Tensor`[], the order of the array needs to\n   * follow the\n   * order of inputNodes array. @see {@link GraphModel.inputNodes}\n   *\n   * You can also feed any intermediate nodes using the NamedTensorMap as the\n   * input type. For example, given the graph\n   *    InputNode => Intermediate => OutputNode,\n   * you can execute the subgraph Intermediate => OutputNode by calling\n   *    model.execute('IntermediateNode' : tf.tensor(...));\n   *\n   * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n   * state needs to be fed manually.\n   *\n   * For batch inference execution, the tensors for each input need to be\n   * concatenated together. For example with mobilenet, the required input shape\n   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n   * If we are provide a batched data of 100 images, the input tensor should be\n   * in the shape of [100, 244, 244, 3].\n   *\n   * @param config Prediction configuration for specifying the batch size.\n   * Currently the batch size option is ignored for graph model.\n   *\n   * @returns Inference result tensors. If the model is converted and it\n   * originally had structured_outputs in tensorflow, then a NamedTensorMap\n   * will be returned matching the structured_outputs. If no structured_outputs\n   * are present, the output will be single `tf.Tensor` if the model has single\n   * output node, otherwise Tensor[].\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  predict(inputs, config) {\n    const outputTensors = this.execute(inputs, this.outputNodes);\n\n    if (this.structuredOutputKeys) {\n      const outputTensorsArray = outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n      const outputTensorMap = {};\n      outputTensorsArray.forEach((outputTensor, i) => outputTensorMap[this.structuredOutputKeys[i]] = outputTensor);\n      return outputTensorMap;\n    }\n\n    return outputTensors;\n  }\n\n  normalizeInputs(inputs) {\n    if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n      // The input is already a NamedTensorMap.\n      return inputs;\n    }\n\n    inputs = Array.isArray(inputs) ? inputs : [inputs];\n\n    if (inputs.length !== this.inputNodes.length) {\n      throw new Error('Input tensor count mismatch,' + `the graph model has ${this.inputNodes.length} placeholders, ` + `while there are ${inputs.length} input tensors.`);\n    }\n\n    return this.inputNodes.reduce((map, inputName, i) => {\n      map[inputName] = inputs[i];\n      return map;\n    }, {});\n  }\n\n  normalizeOutputs(outputs) {\n    outputs = outputs || this.outputNodes;\n    return !Array.isArray(outputs) ? [outputs] : outputs;\n  }\n  /**\n   * Executes inference for the model for given input tensors.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the Tensorflow model, if no\n   * outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   *\n   * @returns A single tensor if provided with a single output or no outputs\n   * are provided and there is only one default output, otherwise return a\n   * tensor array. The order of the tensor array is the same as the outputs\n   * if provided, otherwise the order of outputNodes attribute of the model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  execute(inputs, outputs) {\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = this.executor.execute(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n  /**\n   * Executes inference for the model for given input tensors in async\n   * fashion, use this method when your model contains control flow ops.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   *\n   * @returns A Promise of single tensor if provided with a single output or\n   * no outputs are provided and there is only one default output, otherwise\n   * return a tensor map.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  async executeAsync(inputs, outputs) {\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = await this.executor.executeAsync(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n  /**\n   * Get intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  getIntermediateTensors() {\n    return this.executor.getIntermediateTensors();\n  }\n  /**\n   * Dispose intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  disposeIntermediateTensors() {\n    this.executor.disposeIntermediateTensors();\n  }\n\n  convertTensorMapToTensorsMap(map) {\n    return Object.keys(map).reduce((newMap, key) => {\n      newMap[key] = [map[key]];\n      return newMap;\n    }, {});\n  }\n  /**\n   * Releases the memory used by the weight tensors and resourceManager.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n\n\n  dispose() {\n    this.executor.dispose();\n\n    if (this.initializer) {\n      this.initializer.dispose();\n    }\n\n    this.resourceManager.dispose();\n  }\n\n}\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\n\nexport async function loadGraphModel(modelUrl) {\n  let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  let tfio = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : io;\n\n  if (modelUrl == null) {\n    throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n  }\n\n  if (options == null) {\n    options = {};\n  }\n\n  if (options.fromTFHub && typeof modelUrl === 'string') {\n    modelUrl = getTFHubUrl(modelUrl);\n  }\n\n  const model = new GraphModel(modelUrl, options, tfio);\n  await model.load();\n  return model;\n}\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\n\nexport function loadGraphModelSync(modelSource) {\n  if (modelSource == null) {\n    throw new Error('modelUrl in loadGraphModelSync() cannot be null. Please provide a ' + 'url or an IOHandler that loads the model');\n  }\n\n  if (!modelSource.load) {\n    throw new Error(`modelUrl IO Handler ${modelSource} has no load function`);\n  }\n\n  const model = new GraphModel(modelSource);\n  model.load();\n  return model;\n}\n\nfunction getTFHubUrl(modelUrl) {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = modelUrl + '/';\n  }\n\n  return `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n}","map":{"version":3,"sources":["../../../../../../tfjs-converter/src/executor/graph_model.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAwB,EAAxB,EAAgE,MAAhE,EAAwE,IAAxE,QAAmF,uBAAnF;AAIA,SAAQ,eAAR,QAA8B,gCAA9B;AAEA,SAAQ,aAAR,QAA4B,kBAA5B;AACA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,OAAO,MAAM,kBAAkB,GAAG,mBAA3B;AACP,OAAO,MAAM,kBAAkB,GAAG,YAA3B;AAIP;;;;;;;;;AASG;;AACH,OAAM,MAAO,UAAP,CAAiB;EAiDrB;;;;;;;;AAQG;EACH,WAAA,CACY,QADZ,EAEa;IAAA,IAD2B,WAC3B,uEADyD,EACzD;IAAA,IAAT,IAAS,uEAAF,EAAE;IADD,KAAA,QAAA,GAAA,QAAA;IAA4B,KAAA,WAAA,GAAA,WAAA;IAxDhC,KAAA,OAAA,GAAU,KAAV;IA0DN,KAAK,EAAL,GAAU,IAAV;;IACA,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,KAAK,WAAL,GAAmB,EAAnB;IACD;;IACD,KAAK,eAAL,GAAuB,IAAI,eAAJ,EAAvB;EACD,CAlEoB,CAYrB;;;EACgB,IAAZ,YAAY,GAAA;IACd,OAAO,KAAK,OAAZ;EACD;;EAEa,IAAV,UAAU,GAAA;IACZ,OAAO,KAAK,QAAL,CAAc,UAArB;EACD;;EAEc,IAAX,WAAW,GAAA;IACb,OAAO,KAAK,QAAL,CAAc,WAArB;EACD;;EAES,IAAN,MAAM,GAAA;IACR,OAAO,KAAK,QAAL,CAAc,MAArB;EACD;;EAEU,IAAP,OAAO,GAAA;IACT,OAAO,KAAK,QAAL,CAAc,OAArB;EACD;;EAEU,IAAP,OAAO,GAAA;IACT,OAAO,KAAK,QAAL,CAAc,SAArB;EACD;;EAEW,IAAR,QAAQ,GAAA;IACV,OAAO,KAAK,SAAL,CAAe,mBAAtB;EACD;;EAEiB,IAAd,cAAc,GAAA;IAChB,OAAO,KAAK,SAAZ;EACD;;EAE4B,IAAzB,yBAAyB,GAAA;IAC3B,OAAO,KAAK,oBAAZ;EACD;;EAqBO,aAAa,GAAA;IAEnB,MAAM,IAAI,GAAG,KAAK,QAAlB;;IACA,IAAK,IAAqB,CAAC,IAAtB,IAA8B,IAAnC,EAAyC;MACvC;MACA,KAAK,OAAL,GAAe,IAAf;IACD,CAHD,MAGO,IAAI,KAAK,WAAL,CAAiB,WAAjB,IAAgC,IAApC,EAA0C;MAC/C,KAAK,OAAL,GAAe,KAAK,EAAL,CAAQ,kBAAR,CACI,IADJ,EACoB,KAAK,WADzB,CAAf;IAED,CAHM,MAGA;MACL,MAAM,QAAQ,GACV,KAAK,EAAL,CAAQ,eAAR,CAAwB,IAAxB,EAAwC,KAAK,WAA7C,CADJ;;MAEA,IAAI,QAAQ,CAAC,MAAT,KAAoB,CAAxB,EAA2B;QACzB;QACA;QACA,QAAQ,CAAC,IAAT,CACI,KAAK,EAAL,CAAQ,kBAAR,CAA2B,IAA3B,EAA2C,KAAK,WAAhD,CADJ;MAED,CALD,MAKO,IAAI,QAAQ,CAAC,MAAT,GAAkB,CAAtB,EAAyB;QAC9B,MAAM,IAAI,KAAJ,CACF,wBAAwB,QAAQ,CAAC,MAAM,sBAAvC,GACA,QAAQ,CAAC,IAAD,CAAM,GAFZ,CAAN;MAGD;;MACD,KAAK,OAAL,GAAe,QAAQ,CAAC,CAAD,CAAvB;IACD;EACF;EAED;;;AAGG;;;EACH,IAAI,GAAA;IAGF,KAAK,aAAL;;IACA,IAAI,KAAK,OAAL,CAAa,IAAb,IAAqB,IAAzB,EAA+B;MAC7B,MAAM,IAAI,KAAJ,CACF,sEACA,8CAFE,CAAN;IAGD;;IAKD,MAAM,UAAU,GAAG,KAAK,OAAL,CAAa,IAAb,EAAnB;;IACA,IAAI,IAAI,CAAC,SAAL,CAAe,UAAf,CAAJ,EAAgC;MAC9B,OAAO,UAAU,CAAC,IAAX,CAAgB,SAAS,IAAI,KAAK,QAAL,CAAc,SAAd,CAA7B,CAAP;IACD;;IAED,OAAO,KAAK,QAAL,CAAc,UAAd,CAAP;EACD;EAED;;;;;AAKG;;;EACH,QAAQ,CAAC,SAAD,EAA6B;IACnC,KAAK,SAAL,GAAiB,SAAjB;IACA,MAAM,KAAK,GAAG,KAAK,SAAL,CAAe,aAA7B;IAEA,IAAI,SAAS,GAAG,KAAK,SAAL,CAAe,SAA/B;;IACA,IAAI,KAAK,SAAL,CAAe,mBAAf,IAAsC,IAA1C,EAAgD;MAC9C,MAAM,QAAQ,GAAG,KAAK,SAAL,CAAe,mBAAhC;;MACA,IAAI,QAAQ,CAAC,SAAT,IAAsB,IAA1B,EAAgC;QAC9B,SAAS,GAAG,QAAQ,CAAC,SAArB;MACD;;MAED,IAAI,QAAQ,CAAC,oBAAT,IAAiC,IAArC,EAA2C;QACzC,KAAK,oBAAL,GAA4B,QAAQ,CAAC,oBAArC;MACD;IACF;;IACD,KAAK,SAAL,GAAiB,SAAjB;IAEA,KAAK,OAAL,GAAe,GAAG,KAAK,CAAC,QAAN,CAAe,QAAQ,IAAI,KAAK,CAAC,QAAN,CAAe,WAAW,EAAvE;IACA,MAAM,SAAS,GAAG,KAAK,EAAL,CAAQ,aAAR,CACd,KAAK,SAAL,CAAe,UADD,EACa,KAAK,SAAL,CAAe,WAD5B,CAAlB;IAEA,KAAK,QAAL,GAAgB,IAAI,aAAJ,CACZ,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,KAAxC,EAA+C,KAAK,SAApD,CADY,CAAhB;IAEA,KAAK,QAAL,CAAc,SAAd,GAA0B,KAAK,4BAAL,CAAkC,SAAlC,CAA1B,CAtBmC,CAuBnC;IACA;;IACA,KAAK,QAAL,CAAc,eAAd,GAAgC,KAAK,eAArC;;IAEA,IAAI,SAAS,CAAC,gBAAV,IAA8B,IAA9B,IACC,SAAS,CAAC,gBAAV,CAAoD,IAApD,IAA4D,IADjE,EACuE;MACrE,MAAM,WAAW,GACb,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,SAAS,CAAC,gBAAlD,CADJ;MAEA,KAAK,WAAL,GAAmB,IAAI,aAAJ,CAAkB,WAAlB,CAAnB;MACA,KAAK,WAAL,CAAiB,SAAjB,GAA6B,KAAK,QAAL,CAAc,SAA3C,CAJqE,CAKrE;MACA;MACA;;MACA,KAAK,WAAL,CAAiB,eAAjB,GAAmC,KAAK,eAAxC;MACA,KAAK,WAAL,CAAiB,YAAjB,CAA8B,EAA9B,EAAkC,EAAlC;IACD;;IAED,OAAO,IAAP;EACD;EAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2CG;;;EACO,MAAJ,IAAI,CAAC,YAAD,EAAoC,MAApC,EAA0D;IAElE,IAAI,OAAO,YAAP,KAAwB,QAA5B,EAAsC;MACpC,MAAM,QAAQ,GAAG,KAAK,EAAL,CAAQ,eAAR,CAAwB,YAAxB,CAAjB;;MACA,IAAI,QAAQ,CAAC,MAAT,KAAoB,CAAxB,EAA2B;QACzB,MAAM,IAAI,KAAJ,CACF,0CAA0C,YAAY,GADpD,CAAN;MAED,CAHD,MAGO,IAAI,QAAQ,CAAC,MAAT,GAAkB,CAAtB,EAAyB;QAC9B,MAAM,IAAI,KAAJ,CACF,wBAAwB,QAAQ,CAAC,MAAM,sBAAvC,GACA,QAAQ,YAAY,GAFlB,CAAN;MAGD;;MACD,YAAY,GAAG,QAAQ,CAAC,CAAD,CAAvB;IACD;;IACD,IAAI,YAAY,CAAC,IAAb,IAAqB,IAAzB,EAA+B;MAC7B,MAAM,IAAI,KAAJ,CACF,4DACA,sDAFE,CAAN;IAGD;;IAED,OAAO,YAAY,CAAC,IAAb,CAAkB,KAAK,SAAvB,CAAP;EACD;EAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAsCG;;;EACH,OAAO,CAAC,MAAD,EAAyC,MAAzC,EAAoE;IAEzE,MAAM,aAAa,GAAG,KAAK,OAAL,CAAa,MAAb,EAAqB,KAAK,WAA1B,CAAtB;;IACA,IAAI,KAAK,oBAAT,EAA+B;MAC7B,MAAM,kBAAkB,GACpB,aAAa,YAAY,MAAzB,GAAkC,CAAC,aAAD,CAAlC,GAAoD,aADxD;MAEA,MAAM,eAAe,GAAmB,EAAxC;MAEA,kBAAkB,CAAC,OAAnB,CACI,CAAC,YAAD,EAAe,CAAf,KAAqB,eAAe,CAAC,KAAK,oBAAL,CAA0B,CAA1B,CAAD,CAAf,GACjB,YAFR;MAIA,OAAO,eAAP;IACD;;IACD,OAAO,aAAP;EACD;;EAEO,eAAe,CAAC,MAAD,EACe;IACpC,IAAI,EAAE,MAAM,YAAY,MAApB,KAA+B,CAAC,KAAK,CAAC,OAAN,CAAc,MAAd,CAApC,EAA2D;MACzD;MACA,OAAO,MAAP;IACD;;IACD,MAAM,GAAG,KAAK,CAAC,OAAN,CAAc,MAAd,IAAwB,MAAxB,GAAiC,CAAC,MAAD,CAA1C;;IACA,IAAI,MAAM,CAAC,MAAP,KAAkB,KAAK,UAAL,CAAgB,MAAtC,EAA8C;MAC5C,MAAM,IAAI,KAAJ,CACF,iCACA,uBAAuB,KAAK,UAAL,CAAgB,MAAM,iBAD7C,GAEA,mBAAmB,MAAM,CAAC,MAAM,iBAH9B,CAAN;IAID;;IACD,OAAO,KAAK,UAAL,CAAgB,MAAhB,CAAuB,CAAC,GAAD,EAAM,SAAN,EAAiB,CAAjB,KAAsB;MAClD,GAAG,CAAC,SAAD,CAAH,GAAkB,MAAmB,CAAC,CAAD,CAArC;MACA,OAAO,GAAP;IACD,CAHM,EAGJ,EAHI,CAAP;EAID;;EAEO,gBAAgB,CAAC,OAAD,EAAyB;IAC/C,OAAO,GAAG,OAAO,IAAI,KAAK,WAA1B;IACA,OAAO,CAAC,KAAK,CAAC,OAAN,CAAc,OAAd,CAAD,GAA0B,CAAC,OAAD,CAA1B,GAAsC,OAA7C;EACD;EAED;;;;;;;;;;;;;;;AAeG;;;EACH,OAAO,CAAC,MAAD,EAAyC,OAAzC,EAAkE;IAEvE,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;IACA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;IACA,MAAM,MAAM,GAAG,KAAK,QAAL,CAAc,OAAd,CAAsB,MAAtB,EAA8B,OAA9B,CAAf;IACA,OAAO,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CAA1C;EACD;EACD;;;;;;;;;;;;;;;AAeG;;;EACe,MAAZ,YAAY,CACd,MADc,EAEd,OAFc,EAEW;IAC3B,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;IACA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;IACA,MAAM,MAAM,GAAG,MAAM,KAAK,QAAL,CAAc,YAAd,CAA2B,MAA3B,EAAmC,OAAnC,CAArB;IACA,OAAO,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CAA1C;EACD;EAED;;;;;AAKG;;;EACH,sBAAsB,GAAA;IACpB,OAAO,KAAK,QAAL,CAAc,sBAAd,EAAP;EACD;EAED;;;;;AAKG;;;EACH,0BAA0B,GAAA;IACxB,KAAK,QAAL,CAAc,0BAAd;EACD;;EAEO,4BAA4B,CAAC,GAAD,EAAoB;IACtD,OAAO,MAAM,CAAC,IAAP,CAAY,GAAZ,EAAiB,MAAjB,CAAwB,CAAC,MAAD,EAA0B,GAA1B,KAAiC;MAC9D,MAAM,CAAC,GAAD,CAAN,GAAc,CAAC,GAAG,CAAC,GAAD,CAAJ,CAAd;MACA,OAAO,MAAP;IACD,CAHM,EAGJ,EAHI,CAAP;EAID;EAED;;;;AAIG;;;EACH,OAAO,GAAA;IACL,KAAK,QAAL,CAAc,OAAd;;IAEA,IAAI,KAAK,WAAT,EAAsB;MACpB,KAAK,WAAL,CAAiB,OAAjB;IACD;;IAED,KAAK,eAAL,CAAqB,OAArB;EACD;;AAnZoB;AAsZvB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8BG;;AACH,OAAO,eAAe,cAAf,CACH,QADG,EAEM;EAAA,IADsB,OACtB,uEADgD,EAChD;EAAA,IAAT,IAAS,uEAAF,EAAE;;EACX,IAAI,QAAQ,IAAI,IAAhB,EAAsB;IACpB,MAAM,IAAI,KAAJ,CACF,uEACA,sCAFE,CAAN;EAGD;;EACD,IAAI,OAAO,IAAI,IAAf,EAAqB;IACnB,OAAO,GAAG,EAAV;EACD;;EAED,IAAI,OAAO,CAAC,SAAR,IAAqB,OAAO,QAAP,KAAoB,QAA7C,EAAuD;IACrD,QAAQ,GAAG,WAAW,CAAC,QAAD,CAAtB;EACD;;EACD,MAAM,KAAK,GAAG,IAAI,UAAJ,CAAe,QAAf,EAAyB,OAAzB,EAAkC,IAAlC,CAAd;EACA,MAAM,KAAK,CAAC,IAAN,EAAN;EACA,OAAO,KAAP;AACD;AAED;;;;;;AAMG;;AAEH,OAAM,SAAU,kBAAV,CAA6B,WAA7B,EAA0D;EAE9D,IAAI,WAAW,IAAI,IAAnB,EAAyB;IACvB,MAAM,IAAI,KAAJ,CACF,uEACA,0CAFE,CAAN;EAGD;;EACD,IAAI,CAAC,WAAW,CAAC,IAAjB,EAAuB;IACrB,MAAM,IAAI,KAAJ,CAAU,uBAAuB,WAAW,uBAA5C,CAAN;EACD;;EACD,MAAM,KAAK,GAAG,IAAI,UAAJ,CAAe,WAAf,CAAd;EAEA,KAAK,CAAC,IAAN;EACA,OAAO,KAAP;AACD;;AAED,SAAS,WAAT,CAAqB,QAArB,EAAqC;EACnC,IAAI,CAAC,QAAQ,CAAC,QAAT,CAAkB,GAAlB,CAAL,EAA6B;IAC3B,QAAQ,GAAI,QAAD,GAAa,GAAxB;EACD;;EACD,OAAO,GAAG,QAAQ,GAAG,kBAAkB,GAAG,kBAAkB,EAA5D;AACD","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {InferenceModel, io, ModelPredictConfig, NamedTensorMap, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport * as tensorflow from '../data/compiled_api';\nimport {NamedTensorsMap, TensorInfo} from '../data/types';\nimport {OperationMapper} from '../operations/operation_mapper';\n\nimport {GraphExecutor} from './graph_executor';\nimport {ResourceManager} from './resource_manager';\n\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\ntype Url = string|io.IOHandler|io.IOHandlerSync;\ntype UrlIOHandler<T extends Url> = T extends string ? io.IOHandler : T;\n\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class GraphModel<ModelURL extends Url = string | io.IOHandler> implements\n    InferenceModel {\n  private executor: GraphExecutor;\n  private version = 'n/a';\n  private handler: UrlIOHandler<ModelURL>;\n  private artifacts: io.ModelArtifacts;\n  private initializer: GraphExecutor;\n  private resourceManager: ResourceManager;\n  private signature: tensorflow.ISignatureDef;\n  private structuredOutputKeys: string[];\n  private readonly io: typeof io;\n\n  // Returns the version information for the tensorflow model GraphDef.\n  get modelVersion(): string {\n    return this.version;\n  }\n\n  get inputNodes(): string[] {\n    return this.executor.inputNodes;\n  }\n\n  get outputNodes(): string[] {\n    return this.executor.outputNodes;\n  }\n\n  get inputs(): TensorInfo[] {\n    return this.executor.inputs;\n  }\n\n  get outputs(): TensorInfo[] {\n    return this.executor.outputs;\n  }\n\n  get weights(): NamedTensorsMap {\n    return this.executor.weightMap;\n  }\n\n  get metadata(): {} {\n    return this.artifacts.userDefinedMetadata;\n  }\n\n  get modelSignature(): {} {\n    return this.signature;\n  }\n\n  get modelStructuredOutputKeys(): {} {\n    return this.structuredOutputKeys;\n  }\n\n  /**\n   * @param modelUrl url for the model, or an `io.IOHandler`.\n   * @param weightManifestUrl url for the weight file generated by\n   * scripts/convert.py script.\n   * @param requestOption options for Request, which allows to send credentials\n   * and custom headers.\n   * @param onProgress Optional, progress callback function, fired periodically\n   * before the load is completed.\n   */\n  constructor(\n      private modelUrl: ModelURL, private loadOptions: io.LoadOptions = {},\n      tfio = io) {\n    this.io = tfio;\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n    this.resourceManager = new ResourceManager();\n  }\n\n  private findIOHandler() {\n    type IOHandler = UrlIOHandler<ModelURL>;\n    const path = this.modelUrl;\n    if ((path as io.IOHandler).load != null) {\n      // Path is an IO Handler.\n      this.handler = path as IOHandler;\n    } else if (this.loadOptions.requestInit != null) {\n      this.handler = this.io.browserHTTPRequest(\n                         path as string, this.loadOptions) as IOHandler;\n    } else {\n      const handlers =\n          this.io.getLoadHandlers(path as string, this.loadOptions);\n      if (handlers.length === 0) {\n        // For backward compatibility: if no load handler can be found,\n        // assume it is a relative http path.\n        handlers.push(\n            this.io.browserHTTPRequest(path as string, this.loadOptions));\n      } else if (handlers.length > 1) {\n        throw new Error(\n            `Found more than one (${handlers.length}) load handlers for ` +\n            `URL '${[path]}'`);\n      }\n      this.handler = handlers[0] as IOHandler;\n    }\n  }\n\n  /**\n   * Loads the model and weight files, construct the in memory weight map and\n   * compile the inference graph.\n   */\n  load(): UrlIOHandler<ModelURL> extends io.IOHandlerSync? boolean:\n                                             Promise<boolean> {\n    type IOHandler = UrlIOHandler<ModelURL>;\n    this.findIOHandler();\n    if (this.handler.load == null) {\n      throw new Error(\n          'Cannot proceed with model loading because the IOHandler provided ' +\n          'does not have the `load` method implemented.');\n    }\n\n    type Result =\n        IOHandler extends io.IOHandlerSync ? boolean : Promise<boolean>;\n\n    const loadResult = this.handler.load() as ReturnType<IOHandler['load']>;\n    if (util.isPromise(loadResult)) {\n      return loadResult.then(artifacts => this.loadSync(artifacts)) as Result;\n    }\n\n    return this.loadSync(loadResult) as Result;\n  }\n\n  /**\n   * Synchronously construct the in memory weight map and\n   * compile the inference graph. Also initialize hashtable if any.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  loadSync(artifacts: io.ModelArtifacts) {\n    this.artifacts = artifacts;\n    const graph = this.artifacts.modelTopology as tensorflow.IGraphDef;\n\n    let signature = this.artifacts.signature;\n    if (this.artifacts.userDefinedMetadata != null) {\n      const metadata = this.artifacts.userDefinedMetadata;\n      if (metadata.signature != null) {\n        signature = metadata.signature;\n      }\n\n      if (metadata.structuredOutputKeys != null) {\n        this.structuredOutputKeys = metadata.structuredOutputKeys as string[];\n      }\n    }\n    this.signature = signature;\n\n    this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n    const weightMap = this.io.decodeWeights(\n        this.artifacts.weightData, this.artifacts.weightSpecs);\n    this.executor = new GraphExecutor(\n        OperationMapper.Instance.transformGraph(graph, this.signature));\n    this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n    // Attach a model-level resourceManager to each executor to share resources,\n    // such as `HashTable`.\n    this.executor.resourceManager = this.resourceManager;\n\n    if (artifacts.modelInitializer != null &&\n        (artifacts.modelInitializer as tensorflow.IGraphDef).node != null) {\n      const initializer =\n          OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n      this.initializer = new GraphExecutor(initializer);\n      this.initializer.weightMap = this.executor.weightMap;\n      // Attach a model-level resourceManager to the initializer, the\n      // hashTables created from when executing the initializer will be stored\n      // in the resourceManager.\n      this.initializer.resourceManager = this.resourceManager;\n      this.initializer.executeAsync({}, []);\n    }\n\n    return true;\n  }\n\n  /**\n   * Save the configuration and/or weights of the GraphModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const modelUrl =\n   *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n   * const model = await tf.loadGraphModel(modelUrl);\n   * const zeros = tf.zeros([1, 224, 224, 3]);\n   * model.predict(zeros).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * model.predict(zeros).print();\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL: io.IOHandler|string, config?: io.SaveConfig):\n      Promise<io.SaveResult> {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = this.io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new Error(\n            `Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new Error(\n            `Found more than one (${handlers.length}) save handlers for ` +\n            `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new Error(\n          'GraphModel.save() cannot proceed because the IOHandler ' +\n          'provided does not have the `save` attribute defined.');\n    }\n\n    return handlerOrURL.save(this.artifacts);\n  }\n\n  /**\n   * Execute the inference for the input tensors.\n   *\n   * @param input The input tensors, when there is single input for the model,\n   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n   * inputs params should be in either `tf.Tensor`[] if the input order is\n   * fixed, or otherwise NamedTensorMap format.\n   *\n   * For model with multiple inputs, we recommend you use NamedTensorMap as the\n   * input type, if you use `tf.Tensor`[], the order of the array needs to\n   * follow the\n   * order of inputNodes array. @see {@link GraphModel.inputNodes}\n   *\n   * You can also feed any intermediate nodes using the NamedTensorMap as the\n   * input type. For example, given the graph\n   *    InputNode => Intermediate => OutputNode,\n   * you can execute the subgraph Intermediate => OutputNode by calling\n   *    model.execute('IntermediateNode' : tf.tensor(...));\n   *\n   * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n   * state needs to be fed manually.\n   *\n   * For batch inference execution, the tensors for each input need to be\n   * concatenated together. For example with mobilenet, the required input shape\n   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n   * If we are provide a batched data of 100 images, the input tensor should be\n   * in the shape of [100, 244, 244, 3].\n   *\n   * @param config Prediction configuration for specifying the batch size.\n   * Currently the batch size option is ignored for graph model.\n   *\n   * @returns Inference result tensors. If the model is converted and it\n   * originally had structured_outputs in tensorflow, then a NamedTensorMap\n   * will be returned matching the structured_outputs. If no structured_outputs\n   * are present, the output will be single `tf.Tensor` if the model has single\n   * output node, otherwise Tensor[].\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(inputs: Tensor|Tensor[]|NamedTensorMap, config?: ModelPredictConfig):\n      Tensor|Tensor[]|NamedTensorMap {\n    const outputTensors = this.execute(inputs, this.outputNodes);\n    if (this.structuredOutputKeys) {\n      const outputTensorsArray =\n          outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n      const outputTensorMap: NamedTensorMap = {};\n\n      outputTensorsArray.forEach(\n          (outputTensor, i) => outputTensorMap[this.structuredOutputKeys[i]] =\n              outputTensor);\n\n      return outputTensorMap;\n    }\n    return outputTensors;\n  }\n\n  private normalizeInputs(inputs: Tensor|Tensor[]|\n                          NamedTensorMap): NamedTensorMap {\n    if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n      // The input is already a NamedTensorMap.\n      return inputs;\n    }\n    inputs = Array.isArray(inputs) ? inputs : [inputs];\n    if (inputs.length !== this.inputNodes.length) {\n      throw new Error(\n          'Input tensor count mismatch,' +\n          `the graph model has ${this.inputNodes.length} placeholders, ` +\n          `while there are ${inputs.length} input tensors.`);\n    }\n    return this.inputNodes.reduce((map, inputName, i) => {\n      map[inputName] = (inputs as Tensor[])[i];\n      return map;\n    }, {} as NamedTensorMap);\n  }\n\n  private normalizeOutputs(outputs: string|string[]): string[] {\n    outputs = outputs || this.outputNodes;\n    return !Array.isArray(outputs) ? [outputs] : outputs;\n  }\n\n  /**\n   * Executes inference for the model for given input tensors.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the Tensorflow model, if no\n   * outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   *\n   * @returns A single tensor if provided with a single output or no outputs\n   * are provided and there is only one default output, otherwise return a\n   * tensor array. The order of the tensor array is the same as the outputs\n   * if provided, otherwise the order of outputNodes attribute of the model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  execute(inputs: Tensor|Tensor[]|NamedTensorMap, outputs?: string|string[]):\n      Tensor|Tensor[] {\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = this.executor.execute(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n  /**\n   * Executes inference for the model for given input tensors in async\n   * fashion, use this method when your model contains control flow ops.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   *\n   * @returns A Promise of single tensor if provided with a single output or\n   * no outputs are provided and there is only one default output, otherwise\n   * return a tensor map.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async executeAsync(\n      inputs: Tensor|Tensor[]|NamedTensorMap,\n      outputs?: string|string[]): Promise<Tensor|Tensor[]> {\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = await this.executor.executeAsync(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n\n  /**\n   * Get intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  getIntermediateTensors(): NamedTensorsMap {\n    return this.executor.getIntermediateTensors();\n  }\n\n  /**\n   * Dispose intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  disposeIntermediateTensors() {\n    this.executor.disposeIntermediateTensors();\n  }\n\n  private convertTensorMapToTensorsMap(map: NamedTensorMap): NamedTensorsMap {\n    return Object.keys(map).reduce((newMap: NamedTensorsMap, key) => {\n      newMap[key] = [map[key]];\n      return newMap;\n    }, {});\n  }\n\n  /**\n   * Releases the memory used by the weight tensors and resourceManager.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  dispose() {\n    this.executor.dispose();\n\n    if (this.initializer) {\n      this.initializer.dispose();\n    }\n\n    this.resourceManager.dispose();\n  }\n}\n\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadGraphModel(\n    modelUrl: string|io.IOHandler, options: io.LoadOptions = {},\n    tfio = io): Promise<GraphModel> {\n  if (modelUrl == null) {\n    throw new Error(\n        'modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\n        'or an IOHandler that loads the model');\n  }\n  if (options == null) {\n    options = {};\n  }\n\n  if (options.fromTFHub && typeof modelUrl === 'string') {\n    modelUrl = getTFHubUrl(modelUrl);\n  }\n  const model = new GraphModel(modelUrl, options, tfio);\n  await model.load();\n  return model;\n}\n\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\n\nexport function loadGraphModelSync(modelSource: io.IOHandlerSync):\n    GraphModel<io.IOHandlerSync> {\n  if (modelSource == null) {\n    throw new Error(\n        'modelUrl in loadGraphModelSync() cannot be null. Please provide a ' +\n        'url or an IOHandler that loads the model');\n  }\n  if (!modelSource.load) {\n    throw new Error(`modelUrl IO Handler ${modelSource} has no load function`);\n  }\n  const model = new GraphModel(modelSource);\n\n  model.load();\n  return model;\n}\n\nfunction getTFHubUrl(modelUrl: string): string {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = (modelUrl) + '/';\n  }\n  return `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n}\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}